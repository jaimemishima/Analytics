{"cells":[{"cell_type":"markdown","source":["# Entrega 3"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Objetivo\n+ Este notebook tem como objetivo responder:\n    + `O que fizeram os clientes darem Churn?`\n    + `Qual a importância dos eventos (push, compra, acesso, entre outros) ao longo da vida do cliente?`\n    + `Dado um comportamento de compra no mês atual, qual será o volume de pedidos no próximo mês?`\n\n+ As bases originais são:\n    + `ORDERS - Informações sobre os pedidos realizados.`\n    + `MARKETING PUSH FULL - Notificações PUSH ao longo de 6 meses (Junho-Dezembro/2019).`\n    + `CUSTOMER SEGMENTATION - Segmentação do cliente.`\n    + `ORDERS WITH COST REVENUE - Informações sobre o pedido relacionado, onde verifica-se se o mesmo gerou custo ou receita.`\n    + `SESSION VISITS - Comportamento de uso do app.`\n\n    \nCriado por Jaime Mishima e Ariel Vicente </br>"],"metadata":{}},{"cell_type":"markdown","source":["**PROBLEMA 1**: Para a pergunta de Churn, optamos por criar um modelo preditivo usando um classificador (ajustando 5 técnicas de Machine Learning - e posteriormente elegendo a(s) melhor(es)). A importância das variáveis no modelo nos ajudaram a responder a pergunta.</br>\n**PROBLEMA 2**: Para o problema de **importância dos eventos**, optamos por fazer uma análise descritiva.</br></br>"],"metadata":{}},{"cell_type":"markdown","source":["**PROBLEMA 3**: Prever a valor total gasto com pedidos (order_total) por cliente no mês seguinte.</br>\n\n**Ideia:** Criar um modelo preditivo para, através de regressão, prever o total gasto por um cliente no próximo mês.\n\n**Motivação:** Pelo fato de se ter uma base histórica em mãos, a ideia foi explorar a possibilidade antecipar uma informação valiosa de alguém já conhecido e, assim, ganhar tempo no desenho de estratégias/ações e na tomada de decisão.\n\n**Impacto potencial para o business:**\n- Otimizar pushs: Sabendo-se o total gastos para os clientes de um determinado perfil, o Ifood consegue antecipar a quantidade ideal de pushes no mês.\n- Otimizar alocação de entregadores: Sabendo-se o total gasto com pedidos e onde esses pedidos serão entregues, o Ifood pode trabalhar na otimização do posicionamento dos entregadores (através de incentivos, por exemplo).\n- Otimizar parcerias com restaurantes: Sabendo-se o total gasto com pedidos, onde esses pedidos serão entregues e qual o tipo de comida (que será entregue e a favorita do comprador), o Ifood ganha informações para estimular/alinhar promoções em determinadas regiões e/ou com determinados tipos de restaurante.\n- Identificar potenciais Churn/Inativos: Com a previsão do total gasto com pedidos, consequentemente antecipa-se também o ifood_status do cliente.\n- Antecipar Marlin tag: Sabendo-se o total gasto com pedidos, o Ifood consegue antecipar a classificação da qualidade do cliente (marlin_tag), podendo criar promoções/ações/pushes específicos para cara um dos tipos de cliente em potencial para fidelizar ainda mais os melhores e estimular (se fizer sentido) a evolução dos demais."],"metadata":{}},{"cell_type":"markdown","source":["## 2. Imports"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf, count, when, isnull, col, mean, sum, max, avg, min, stddev, count, trim, lower, split, explode\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import collect_list\nfrom pyspark.mllib.stat import Statistics\n\n# tratamento de datas\nfrom pyspark.sql.functions import datediff, to_date, to_timestamp, from_utc_timestamp, round, dayofweek, month\n\n# para o groupby e lag column\nimport pyspark.sql.functions as f\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import lit\n\n# para a remoção de missing:\nfrom functools import reduce\n\n# para ajuste de type de arrays\nfrom pyspark.sql.types import ArrayType, StringType\n\n# para correlacao\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Modeling\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, NaiveBayes\nfrom pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor, GeneralizedLinearRegression, RandomForestRegressionModel, GeneralizedLinearRegressionModel\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, roc_curve\nfrom pyspark.ml.tuning import TrainValidationSplit\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, OneHotEncoderEstimator, StringIndexer, VectorAssembler, VectorSlicer, VectorIndexer\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\nfrom pyspark.mllib.util import MLUtils"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## 3. Help Functions"],"metadata":{}},{"cell_type":"code","source":["# Percentual de Missings\nfrom pyspark.sql.functions import count, when, isnull, col\ndef contar_missing(df):\n  \"\"\"Realiza a contagem da quantidade de missing que existe dentro de um dataframe.\n  Args:\n    df - Dataframe Spark\n  Returns:\n    Data Frame spark com apenas 1 linha com a contagem de missing para cada variável.\n  \"\"\"\n  aux = []\n  for c in df.columns:\n    aux.append(count(when(isnull(c), c)).alias(c))\n  return df.select(aux)\n\ndef percent_missing(df):\n  total_linhas = df.count()\n  df = contar_missing(df)\n  colunas = df.columns\n  total_missing = list(df.first().asDict().values())\n  valores = zip(colunas, total_missing)\n  \n  df_aux = spark.createDataFrame(valores, ['variaveis', 'total_missing'])\n  df_aux = df_aux.withColumn('perc_missing', col('total_missing') *100/total_linhas)\n  return df_aux\n\n# Returns a groupby count of a dataframe (df) column (col)\ndef percentByCol(df, group_column):\n  \"\"\"Retorna o groupby de uma coluna `col` de um dataframe `df`\n  Args:\n    df - Dataframe Spark\n    group_column - Nome da coluna do dataframe df\n  Returns:\n    Data Frame spark com o groupby da coluna `col` e uma coluna com o percentual\n  \"\"\"\n  return df.groupby(group_column)\\\n           .count()\\\n           .withColumnRenamed('count', 'cnt_per_group')\\\n           .withColumn('percent', f.col('cnt_per_group')*100/f.sum('cnt_per_group').over(Window.partitionBy()))\\\n           .orderBy('percent', ascending=False)\n\n# Join two dataframes and remove duplicate columns\n# Disclaimer: baseado em https://stackoverflow.com/questions/46944493/removing-duplicate-columns-after-a-df-join-in-spark\ndef join_removing_repeated(df1, df2, cond, how='left'):\n    \"\"\"Retorna o dataframe resultado do join de `df1` e `df2`\n    Args:\n      df1 - dataframe 1\n      df2 - dataframe 2\n      cond - chaves para realizar o join\n      how - tipo de join (default left)\n    Returns:\n      Data Frame resultado do join removendo as colunas repetidas\n    \"\"\"\n    df = df1.join(df2, cond, how=how)\n    repeated_columns = [c for c in df1.columns if c in df2.columns]\n    for col in repeated_columns:\n        df = df.drop(df2[col])\n    return df\n  \n# Clears the received string x from unwanted characters\ndef limpeza(x):\n  \"\"\"Retorna a string x após eliminação de caracteres indesejados\n    Args:\n      x - string\n    Returns:\n      String tratada\n    \"\"\"\n  return x.replace('\"', '').replace('\\\\', '').replace('[', '').replace(']', '')\n\nudf_limpeza = udf(limpeza, StringType()) # create an udf based on limpeza function\n\n# Classifica o número de pushes por ranges\nfrom pyspark.sql.functions import udf\npush_range = udf(lambda pushes: '1- < 20' if pushes < 20 else \n                                 '2- 20-60' if (pushes >= 20 and pushes < 60) else\n                                 '3- 60-100' if (pushes >= 60 and pushes < 100) else\n                                 '4- 100-140' if (pushes >= 100 and pushes < 140) else\n                                 '5- 140-180' if (pushes >= 140 and pushes < 180) else\n                                 '6- 180-220' if (pushes >= 180 and pushes < 220) else\n                                 '7- 220-260' if (pushes >= 220 and pushes < 260) else\n                                 '8- 260-300' if (pushes >= 260 and pushes < 300) else\n                                 '9- 300+'  if (pushes >= 300) else '')\n\n# Buckets por número de pedidos\nfrom pyspark.sql.functions import udf\norder_range = udf(lambda orders: '1- < 5' if orders < 5 else \n                                 '2- 5-10' if (orders >= 5 and orders < 10) else\n                                 '3- 0-15' if (orders >= 10 and orders < 15) else\n                                 '4- 15-20' if (orders >= 15 and orders < 20) else\n                                 '5- 20-25' if (orders >= 20 and orders < 25) else\n                                 '6- 25-30' if (orders >= 25 and orders < 30) else\n                                 '7- 30-35' if (orders >= 30 and orders < 35) else\n                                 '8- 35-40' if (orders >= 35 and orders < 40) else\n                                 '9- 40+'  if (orders >= 40) else '')\n\n# Heatmap matriz de correlação\nimport seaborn as sns\ndef plot_corr_matrix(correlations,attr,fig_no, figsize=[15,10]):\n  \"\"\"Retorna o heatmap de uma lista de listas com as correlacoes de variaveis\n  Args:\n    correlations - lista de lista com as correlacoes\n    attr - lista com os nomes das variaveis\n    fig_no - If not provided, a new figure will be created, and the figure number will be incremented (para o plt.figure)\n    figsize 0- tamanho do heatmap\n  Returns:\n    Heatmap com a matriz de correlacao\n    \"\"\"\n  fig=plt.figure(fig_no, figsize=figsize)\n  ax=fig.add_subplot(111)\n  ax.set_title(\"Correlacao Variaveis\")\n  ax = sns.heatmap(correlations, cmap=\"YlGnBu\")\n  indice = list(range(1, len(attr)+1))\n  indice = [str(s) + ' - ' for s in indice]\n  res = [i + j for i, j in zip(indice, attr)] \n  #ax.set_xticks(range(len(filter_colunas_order)))\n  ax.set_yticklabels(res)\n  plt.yticks(rotation=0) \n  plt.show()\n  \n# Função para computar o MAPE\ndef compute_mape(df, y_true='label', y_pred='prediction'):\n  mape = df.withColumn('abserror', col(y_true) - col(y_pred))\n  mape = mape.withColumn('relerror', abs(col('abserror') / col(y_true)))\n  mape = mape.select(round(mean(col('relerror')) * 100, 4).alias('mape'))\n  \n  return mape.collect()[0][0]\n\n# Mode function to get most frequent values of a dataframe\n# Reference: https://stackoverflow.com/questions/45880089/how-to-get-most-frequent-values-of-a-dataframe-in-pyspark\n@f.udf\ndef most_common_udf(x):\n    from collections import Counter\n    return Counter(x).most_common(1)[0][0]\n\n# Returns feature importances of model\n# Reference: https://www.timlrx.com/2018/06/19/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator/\nimport pandas as pd\ndef ExtractFeatureImp(featureImp, dataset, featuresCol):\n    list_extract = []\n    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n    varlist = pd.DataFrame(list_extract)\n    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n    return(varlist.sort_values('score', ascending = False))\n  \n  \n# Função auxiliar para cálculo e exibição de métricas de avaliação dos modelos de classificação:\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\ndef class_model_evaluator(predict, labelCol=\"target\"):\n  \n  accuracy = MulticlassClassificationEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName='accuracy').evaluate(predict)\n  weightedPrecision = MulticlassClassificationEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName='weightedPrecision').evaluate(predict)\n  weightedRecall = MulticlassClassificationEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName='weightedRecall').evaluate(predict)\n  f1 = MulticlassClassificationEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName='f1').evaluate(predict)\n  \n  areaUnderROC = BinaryClassificationEvaluator(labelCol=labelCol, metricName='areaUnderROC').evaluate(predict)\n\n  print(f'AUC ROC: {areaUnderROC}')\n  print(f'Accuracy: {accuracy}')\n  print(f'Precision: {weightedPrecision}')\n  print(f'Recall: {weightedRecall}')\n  print(f'F1-score: {f1}')\n  \n  if 1==1:\n    pass\n  else:\n    return None\n  \n  \n# Função auxiliar para cálculo e exibição de métricas de avaliação dos modelos de regressão:\nfrom pyspark.ml.evaluation import RegressionEvaluator\ndef reg_model_evaluator(predict, labelCol=\"target\"):\n  \n  mae = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName='mae').evaluate(predict)\n  rmse = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName='rmse').evaluate(predict)\n  r2 = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName='r2').evaluate(predict)\n\n  print(f'MAE: {mae}')\n  print(f'RMSE: {rmse}')\n  print(f'r2 Ajustado: {r2}')\n  if 1==1:\n    pass\n  else:\n    return None\n  \n  \nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\n# Reference: https://stackoverflow.com/questions/52847408/pyspark-extract-roc-curve\n# Scala version implements .roc() and .pr()\n# Python: https://spark.apache.org/docs/latest/api/python/_modules/pyspark/mllib/common.html\n# Scala: https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/evaluation/BinaryClassificationMetrics.html\nclass CurveMetrics(BinaryClassificationMetrics):\n    def __init__(self, *args):\n        super(CurveMetrics, self).__init__(*args)\n\n    def _to_list(self, rdd):\n        points = []\n        # Note this collect could be inefficient for large datasets \n        # considering there may be one probability per datapoint (at most)\n        # The Scala version takes a numBins parameter, \n        # but it doesn't seem possible to pass this from Python to Java\n        for row in rdd.collect():\n            # Results are returned as type scala.Tuple2, \n            # which doesn't appear to have a py4j mapping\n            points += [(float(row._1()), float(row._2()))]\n            \n        # Separa em FPR e TPR\n        fpr = []\n        tpr = []\n        for f, t in points:\n          fpr.append(f)\n          tpr.append(t)\n        \n        return fpr, tpr\n\n    def get_curve(self, method):\n        rdd = getattr(self._java_model, method)().toJavaRDD()\n        return self._to_list(rdd)\n\ndef timeline_plot_sum(vars):\n  \n  t = np.arange(6, 13, 1)\n  ax = []\n  count = 0\n  for var1, var2 in vars:\n    # Create some mock data\n    data_1 = df_desc_filtro.groupby('segmentation_month').agg(sum(var1).alias(var1)).orderBy('segmentation_month')\n    data_2 = df_desc_filtro.groupby('segmentation_month').agg(sum(var2).alias(var2)).orderBy('segmentation_month')\n\n    data1 = np.array(data_1.select(var1).collect())\n    data2 = np.array(data_2.select(var2).collect())\n\n    fig, ax_aux = plt.subplots()\n    ax.append(ax_aux)\n\n    color = 'tab:red'\n    ax[count].set_xlabel('timeline')\n    ax[count].set_ylabel(var1, color=color)\n    ax[count].plot(t, data1, color=color)\n    ax[count].tick_params(axis='y', labelcolor=color)\n\n    ax.append(ax[count].twinx())  # instantiate a second axes that shares the same x-axis\n\n    color = 'tab:blue'\n    ax[count+1].set_ylabel(var2, color=color)  # we already handled the x-label with ax1\n    ax[count+1].plot(t, data2, color=color)\n    ax[count+1].tick_params(axis='y', labelcolor=color)\n    \n    count = count + 2\n  \n  aux = list(range(1, len(charts_list) * 2, 2))\n  ax[1].get_shared_y_axes().join([ax[x] for x in aux])\n\n  fig.tight_layout()  # otherwise the right y-label is slightly clipped\n  plt.show()\n  \n  \ndef timeline_plot_avg(vars):\n  \n  t = np.arange(6, 13, 1)\n  ax = []\n  count = 0\n  for var1, var2 in vars:\n    # Create some mock data\n    data_1 = df_desc_filtro.groupby('segmentation_month').agg(avg(var1).alias(var1)).orderBy('segmentation_month')\n    data_2 = df_desc_filtro.groupby('segmentation_month').agg(avg(var2).alias(var2)).orderBy('segmentation_month')\n\n    data1 = np.array(data_1.select(var1).collect())\n    data2 = np.array(data_2.select(var2).collect())\n\n    fig, ax_aux = plt.subplots()\n    ax.append(ax_aux)\n\n    color = 'tab:red'\n    ax[count].set_xlabel('timeline')\n    ax[count].set_ylabel(var1, color=color)\n    ax[count].plot(t, data1, color=color)\n    ax[count].tick_params(axis='y', labelcolor=color)\n\n    ax.append(ax[count].twinx())  # instantiate a second axes that shares the same x-axis\n\n    color = 'tab:blue'\n    ax[count+1].set_ylabel(var2, color=color)  # we already handled the x-label with ax1\n    ax[count+1].plot(t, data2, color=color)\n    ax[count+1].tick_params(axis='y', labelcolor=color)\n    \n    count = count + 2\n  \n  aux = list(range(1, len(charts_list) * 2, 2))\n  ax[1].get_shared_y_axes().join([ax[x] for x in aux])\n\n  fig.tight_layout()  # otherwise the right y-label is slightly clipped\n  plt.show()\n  \ndef timeline_plot_mixed(vars):\n  \n  t = np.arange(6, 13, 1)\n  ax = []\n  count = 0\n  for var1, var2 in vars:\n    # Create some mock data\n    data_1 = df_desc_filtro.groupby('segmentation_month').agg(sum(var1).alias(\"sum_of_\" + var1)).orderBy('segmentation_month')\n    data_2 = df_desc_filtro.groupby('segmentation_month').agg(avg(var2).alias(\"avg_of_\" + var2)).orderBy('segmentation_month')\n\n    data1 = np.array(data_1.select(\"sum_of_\" + var1).collect())\n    data2 = np.array(data_2.select(\"avg_of_\" + var2).collect())\n\n    fig, ax_aux = plt.subplots()\n    ax.append(ax_aux)\n\n    color = 'tab:red'\n    ax[count].set_xlabel('timeline')\n    ax[count].set_ylabel(\"sum_of_\" + var1, color=color)\n    ax[count].plot(t, data1, color=color)\n    ax[count].tick_params(axis='y', labelcolor=color)\n\n    ax.append(ax[count].twinx())  # instantiate a second axes that shares the same x-axis\n\n    color = 'tab:blue'\n    ax[count+1].set_ylabel(\"avg_of_\" + var2, color=color)  # we already handled the x-label with ax1\n    ax[count+1].plot(t, data2, color=color)\n    ax[count+1].tick_params(axis='y', labelcolor=color)\n    \n    count = count + 2\n  \n  aux = list(range(1, len(charts_list) * 2, 2))\n  ax[1].get_shared_y_axes().join([ax[x] for x in aux])\n\n  fig.tight_layout()  # otherwise the right y-label is slightly clipped\n  plt.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## 4. Base Treatment\nBases Geradas na entrega 1"],"metadata":{}},{"cell_type":"markdown","source":["### 4.1 Read Raw Bases"],"metadata":{}},{"cell_type":"code","source":["root_dir = '/dbfs/FileStore/ifood'\ndbutils.fs.ls(f'{root_dir}')\ndf_customer_segmentation = spark.read.parquet(f'{root_dir}/customer_segmentation')\ndf_orders = spark.read.parquet(f'{root_dir}/orders')\ndf_orders_with_cost_revenue = spark.read.parquet(f'{root_dir}/orders_with_cost_revenue')\ndf_sessions_visits = spark.read.parquet(f'{root_dir}/sessions_visits')\ndf_marketing_push_full = spark.read.parquet(f'{root_dir}/marketing_push_full')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["### 4.2 Treatment"],"metadata":{}},{"cell_type":"code","source":["# ---------------------------\n#          PUSHES\n# ---------------------------\n\n# Missing Values Treatment\ncolunas_pushes_missing = percent_missing(df_marketing_push_full).orderBy('perc_missing', ascending=False)\nthresholdMissingPushes = 22\ndf_columns_to_drop = colunas_pushes_missing.filter(colunas_pushes_missing['perc_missing'] > thresholdMissingPushes).select('variaveis')\nlist_columns_to_drop = list([row[0] for row in df_columns_to_drop.collect()])\ndf_mpf = df_marketing_push_full.drop(*list_columns_to_drop)\ndf_mpf = df_mpf.dropDuplicates()\n\n# Para as colunas que nao remover, separar em categoricas e numericas\ndf_columns_missing = colunas_pushes_missing.filter(\n                                                   (colunas_pushes_missing['perc_missing'] <= thresholdMissingPushes) &\n                                                   (colunas_pushes_missing['perc_missing'] > 0)\n                                                  ).select('variaveis')\nlist_df_columns_missing = list([row[0] for row in df_columns_missing.collect()])\n\nfilter_colunas_numericas = [x[0] for x in df_mpf[list_df_columns_missing].dtypes if x[1] in ('double', 'int', 'long')]\nfilter_colunas_categoricas = [x[0] for x in df_mpf[list_df_columns_missing].dtypes if x[1] not in ('double', 'int', 'long')]\n\n# Inputar unknown para colunas categoricas\nfor coluna in filter_colunas_categoricas:\n  df_mpf = df_mpf.fillna('unknown', subset=[coluna])\n\n# Inputar media para colunas numericas\nfor coluna in filter_colunas_numericas:\n  media = df_mpf.agg(mean(coluna)).collect()[0][0]\n  df_mpf = df_mpf.fillna(media, subset=[coluna])\n\n# Date Treatment\n# importante: transformar primeiro para timestamp, depois para date!!!\ndf_mpf = df_mpf.withColumn('event_time_utc3', from_utc_timestamp('event_time_utc3', 'UTC'))\\\n               .withColumn('event_date', from_utc_timestamp('event_date', 'UTC'))\\\n               .withColumn('event_date', to_date('event_date', 'YYYY-MM-DD'))\\\n               .withColumn('event_month', month('event_date'))\\\n               .withColumn('event_dayofweek', dayofweek('event_date'))\n\n\n# ---------------------------\n#    CUSTOMER SEGMENTATION\n# ---------------------------\n\n# Remove duplicates\ndf_customer_segmentation = df_customer_segmentation.distinct()\n\n# Missing Treatment\nperc_miss = percent_missing(df_customer_segmentation) # Calcula o percentual de missings para todas as colunas\ncol_drop = perc_miss.filter((perc_miss['perc_missing'] <= 0.1) & (perc_miss['perc_missing'] > 0)).select('variaveis').rdd.flatMap(lambda x: x).collect() # Seleciona colunas com menos de 0.1% de missings\ncol_drop.append('customer_id') # Acrescenta a coluna de customer_id na lista de colunas com missing\naux1 = df_customer_segmentation.select(col_drop) # Criar dataframe auxiliar apenas com as colunas em col_drop\n# Cria lista com o customer_id de quem tem algum dado nulo para alguma das colunas em col_drop:\nc_id_drop = aux1.where(reduce(lambda x, y: x | y, (f.col(x).isNull() for x in aux1.columns))).select('customer_id').distinct().rdd.flatMap(lambda x: x).collect() \n# Exclui de df_customer_segmentation todos os registros de quem teve informação nula identificada: (Decisão tomada por conta da base ser histórica e o impacto em número absoluto de resgistros ser pequeno)\ndf_customer_segmentation = df_customer_segmentation.filter(~df_customer_segmentation.customer_id.isin(c_id_drop))\n\n# Array Treatment\n#Ajuste coluna preferred_dishes (tipo array):\ndf_customer_segmentation = df_customer_segmentation.withColumn('aux_1', udf_limpeza(col('preferred_dishes')))\ndf_customer_segmentation = df_customer_segmentation.withColumn('preferred_dishes_ar', split(col('aux_1'), ',').cast(ArrayType(StringType())))\n#Ajuste coluna top_3_merchants_code (tipo array):\ndf_customer_segmentation = df_customer_segmentation.withColumn('aux_2', udf_limpeza(col('top_3_merchants_code')))\ndf_customer_segmentation = df_customer_segmentation.withColumn('top_3_merchants_code_ar', split(col('aux_2'), ',').cast(ArrayType(StringType())))\n\nlist_columns_to_drop = ['aux_1','aux_2']\ndf_customer_segmentation = df_customer_segmentation.drop(*list_columns_to_drop) # Exclusão de colunas auxiliares\n\n# Date Treatment\ndf_customer_segmentation = df_customer_segmentation.withColumn('registration_date', from_utc_timestamp('registration_date', 'UTC'))\\\n                                                   .withColumn('registration_date', to_date('registration_date', 'YYYY-MM-DD'))\\\n                                                   .withColumn('last_valid_order_date', from_utc_timestamp('last_valid_order_date', 'UTC'))\\\n                                                   .withColumn('last_valid_order_date', to_date('last_valid_order_date', 'YYYY-MM-DD'))\\\n                                                   .withColumn('last_invalid_order_date', from_utc_timestamp('last_invalid_order_date', 'UTC'))\\\n                                                   .withColumn('last_invalid_order_date', to_date('last_invalid_order_date', 'YYYY-MM-DD'))\\\n                                                   .withColumn('first_order_date', from_utc_timestamp('first_order_date', 'UTC'))\\\n                                                   .withColumn('first_order_date', to_date('first_order_date', 'YYYY-MM-DD'))\\\n                                                   .withColumn('last_order_date', from_utc_timestamp('last_order_date', 'UTC'))\\\n                                                   .withColumn('last_order_date', to_date('last_order_date', 'YYYY-MM-DD'))\\\n                                                   .withColumn('segmentation_month', from_utc_timestamp('segmentation_month', 'UTC'))\\\n                                                   .withColumn('segmentation_month', to_date('segmentation_month', 'YYYY-MM-DD'))\\\n                                                   .withColumn('registration_month', month('registration_date'))\\\n                                                   .withColumn('registration_dayofweek', dayofweek('registration_date'))\\\n                                                   .withColumn('first_order_month', month('first_order_date'))\\\n                                                   .withColumn('first_order_dayofweek', dayofweek('first_order_date'))\\\n                                                   .withColumn('segmentation_month_month', month('segmentation_month'))\\\n                                                   .withColumn('segmentation_month_dayofweek', dayofweek('segmentation_month'))\\\n                                                   .withColumn('last_order_month', month('last_order_date'))\\\n                                                   .withColumn('last_order_dayofweek', dayofweek('last_order_date'))\n\n\n# ---------------------------\n#  ORDERS + ORDERS w/ COSTS\n# ---------------------------\n\n# Join Orders and Orders with Cost/Revenue bases\ndf_orders_total = join_removing_repeated(df_orders, df_orders_with_cost_revenue, df_orders.order_number == df_orders_with_cost_revenue.order_number, 'left')\n\n# Missing Treatment\ncolunas_orders_missing = percent_missing(df_orders_total).orderBy('perc_missing', ascending=False)\n# Para as colunas com missing, separar em categoricas e numericas\ndf_columns_orders_missing = colunas_orders_missing.filter(colunas_orders_missing['perc_missing'] > 0\n                                                  ).select('variaveis')\nlist_df_columns_orders_missing = list([row[0] for row in df_columns_orders_missing.collect()])\n\nfilter_colunas_orders_numericas = [x[0] for x in df_orders_total[list_df_columns_orders_missing].dtypes if x[1] in ('double', 'long')]\nfilter_colunas_orders_numericas_int = [x[0] for x in df_orders_total[list_df_columns_orders_missing].dtypes if x[1] in ('int')]\nfilter_colunas_orders_categoricas = [x[0] for x in df_orders_total[list_df_columns_orders_missing].dtypes if x[1] not in ('double', 'int', 'long', 'boolean')]\nfilter_colunas_orders_booleanas = [x[0] for x in df_orders_total[list_df_columns_orders_missing].dtypes if x[1] in ('boolean')]\n\n# Inputar unknown para colunas categoricas\nfor coluna in filter_colunas_orders_categoricas:\n  df_orders_total = df_orders_total.fillna('unknown', subset=[coluna])\n\n# Transformar colunas booleanas para string, depois inputar unknown\nfor coluna in filter_colunas_orders_booleanas:\n  df_orders_total = df_orders_total.withColumn(coluna,col(coluna).cast('string'))\n  df_orders_total = df_orders_total.fillna('unknown', subset=[coluna])\n\n# Inputar media para colunas numericas\nfor coluna in filter_colunas_orders_numericas:\n  media = df_orders_total.agg(mean(coluna)).collect()[0][0]\n  df_orders_total = df_orders_total.fillna(media, subset=[coluna])\n  \n# Inputar media para colunas numericas inteiras arredondando para o integer mais perto\nfor coluna in filter_colunas_orders_numericas:\n  media = df_orders_total.agg(mean(coluna)).collect()[0][0]\n  df_orders_total = df_orders_total.fillna(media, subset=[coluna])\n  df_orders_total = df_orders_total.withColumn(coluna, f.round(df_orders_total[coluna], 0))\n\n# Date Treatment  \n# importante: transformar primeiro para timestamp, depois para date!!!\ndf_orders_total = df_orders_total.withColumn('order_timestamp_local', from_utc_timestamp('order_timestamp_local', 'UTC'))\\\n                                 .withColumn('last_status_date_local', from_utc_timestamp('last_status_date_local', 'UTC'))\\\n                                 .withColumn('scheduled_creation_date_local', from_utc_timestamp('scheduled_creation_date_local', 'UTC'))\\\n                                 .withColumn('order_date_local', from_utc_timestamp('order_date_local', 'UTC'))\\\n                                 .withColumn('cohort_month', from_utc_timestamp('cohort_month', 'UTC'))\\\n                                 .withColumn('first_order_date', from_utc_timestamp('first_order_date', 'UTC'))\\\n                                 .withColumn('order_date_local', to_date('order_date_local', 'YYYY-MM-DD'))\\\n                                 .withColumn('cohort_month', to_date('cohort_month', 'YYYY-MM-DD'))\\\n                                 .withColumn('first_order_date', to_date('first_order_date', 'YYYY-MM-DD'))\\\n                                 .withColumn('order_date_local_month', month('order_date_local'))\\\n                                 .withColumn('order_date_local_dayofweek', dayofweek('order_date_local'))\n\n# ---------------------------\n#           VISITS\n# ---------------------------\n\n# Missing Treatment\ncolunas_visits_missing = percent_missing(df_sessions_visits).orderBy('perc_missing', ascending=False)\n# Para as colunas com missing, separar em categoricas e numericas\ndf_columns_visits_missing = colunas_visits_missing.filter(colunas_visits_missing['perc_missing'] > 0\n                                                  ).select('variaveis')\nlist_df_columns_visits_missing = list([row[0] for row in df_columns_visits_missing.collect()])\n\nfilter_colunas_visits_numericas = [x[0] for x in df_sessions_visits[list_df_columns_visits_missing].dtypes if x[1] in ('double', 'long')]\nfilter_colunas_visits_numericas_int = [x[0] for x in df_sessions_visits[list_df_columns_visits_missing].dtypes if x[1] in ('int')]\nfilter_colunas_visits_categoricas = [x[0] for x in df_sessions_visits[list_df_columns_visits_missing].dtypes if x[1] not in ('double', 'int', 'long', 'boolean')]\nfilter_colunas_visits_booleanas = [x[0] for x in df_sessions_visits[list_df_columns_visits_missing].dtypes if x[1] in ('boolean')]\n\n# Inputar unknown para colunas categoricas\nfor coluna in filter_colunas_visits_categoricas:\n  df_sessions_visits = df_sessions_visits.fillna('unknown', subset=[coluna])\n\n# Transformar colunas booleanas para string, depois inputar unknown\nfor coluna in filter_colunas_visits_booleanas:\n  df_sessions_visits = df_sessions_visits.withColumn(coluna,col(coluna).cast('string'))\n  df_sessions_visits = df_sessions_visits.fillna('unknown', subset=[coluna])\n  \n# Inputar media para colunas numericas inteiras arredondando para o integer mais perto\nfor coluna in filter_colunas_visits_numericas:\n  media = df_sessions_visits.agg(mean(coluna)).collect()[0][0]\n  df_sessions_visits = df_sessions_visits.fillna(media, subset=[coluna])\n  df_sessions_visits = df_sessions_visits.withColumn(coluna, f.round(df_sessions_visits[coluna], 0))\n  \n\n# Dates Conversion\ndf_sessions_visits = df_sessions_visits.withColumn('session_started_at_amsp', from_utc_timestamp('session_started_at_amsp', 'UTC'))\\\n                                       .withColumn('session_ended_at_amsp', from_utc_timestamp('session_ended_at_amsp', 'UTC'))\\\n                                       .withColumn('session_started_at_utc0', from_utc_timestamp('session_started_at_utc0', 'UTC'))\\\n                                       .withColumn('session_ended_at_utc0', from_utc_timestamp('session_ended_at_utc0', 'UTC'))\\\n                                       .withColumn('session_started_at_utc0', from_utc_timestamp('session_started_at_utc0', 'UTC'))\\\n                                       .withColumn('session_started_date', to_date('session_started_at_amsp', 'YYYY-MM-DD'))\\\n                                       .withColumn('session_started_month', month('session_started_date'))\\\n                                       .withColumn('session_started_dayofweek', dayofweek('session_started_date'))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### 4.3 Save Tables"],"metadata":{}},{"cell_type":"code","source":["%fs rm -r /dbfs/FileStore/treated/"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Create folder\ntreat_dir = '/dbfs/FileStore/treated'\ndbutils.fs.mkdirs(f'{treat_dir}')\n\ndf_mpf.write.parquet('/dbfs/FileStore/treated/df_mpf.parquet')\ndf_customer_segmentation.write.parquet('/dbfs/FileStore/treated/df_customer_segmentation.parquet')\ndf_orders_total.write.parquet('/dbfs/FileStore/treated/df_orders_total.parquet')\ndf_sessions_visits.write.parquet('/dbfs/FileStore/treated/df_sessions_visits.parquet')"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## 5. ABT Generation"],"metadata":{}},{"cell_type":"code","source":["# Read from Parquet\nroot_dir = '/dbfs/FileStore/treated'\ndbutils.fs.ls(f'{root_dir}')\ndf_customer_segmentation = spark.read.parquet(f'{root_dir}/df_customer_segmentation.parquet')\ndf_orders_total_tratado = spark.read.parquet(f'{root_dir}/df_orders_total.parquet')\ndf_sessions_visits_tratado = spark.read.parquet(f'{root_dir}/df_sessions_visits.parquet')\ndf_mpf_tratado = spark.read.parquet(f'{root_dir}/df_mpf.parquet')\n\n### Pushes Grouped by 'external_user_id' and 'event_month'\ngrouping_cols = [\"external_user_id\", \"event_month\"]\nother_cols = [c for c in df_mpf_tratado.columns if c not in grouping_cols]\nexclude_cols = ['event_channel', 'event_name', 'brand', 'sample_type', 'user_id', 'campaign_id', 'event_dayofweek', 'message_variation_channel']\nmpf_cols = list(set(other_cols) - set(exclude_cols))\n\n# Group Pushes base counting distinct events of a customer in a month\ndf_mpf_grouped = df_mpf_tratado.filter(df_mpf_tratado['event_name'] == 'received')\\\n                               .groupBy(grouping_cols).agg(*[countDistinct(c).alias(\"pushes_count_distinct_\"+c) for c in mpf_cols])\ndf_mpf_grouped = df_mpf_grouped.withColumn('pushes_changed_platform',\\\n                                           when(df_mpf_grouped.pushes_count_distinct_platform >= 2,1).otherwise(0))\ndf_mpf_grouped = df_mpf_grouped[[list(set(df_mpf_grouped.columns) - set(['pushes_count_distinct_platform']))]]\n\n# Most common platform in a month where a user received most pushes\nw = Window.partitionBy(\"external_user_id\", \"event_month\")\nmost_common_platform = df_mpf_tratado.filter(df_mpf_tratado['event_name'] == 'received')\\\n                                     .groupBy(\"external_user_id\", \"event_month\", \"platform\")\\\n                                     .agg(countDistinct(\"event_time_utc3\").alias(\"pushes_count\"))\\\n                                     .withColumn(\"rn\",row_number().over(w.orderBy(col(\"pushes_count\").desc())))\\\n                                     .where(col(\"rn\") == 1)\\\n                                     .drop(\"rn\")\\\n                                     .withColumnRenamed('platform', 'most_common_platform')\\\n                                     .select([\"external_user_id\", \"event_month\", \"most_common_platform\"])\ncond = [df_mpf_grouped.external_user_id == most_common_platform.external_user_id, df_mpf_grouped.event_month == most_common_platform.event_month]\ndf_mpf_grouped = join_removing_repeated(df_mpf_grouped, most_common_platform, cond, 'left')\n\n\n\n### Orders and Sessions join via session_id\ndf_orders_and_sessions = join_removing_repeated(df_orders_total_tratado, df_sessions_visits_tratado, df_orders_total_tratado.session_id == df_sessions_visits_tratado.session_id, 'left')\ndf_orders_and_sessions = df_orders_and_sessions.withColumn('order_timestamp_local', from_utc_timestamp('order_timestamp_local', 'UTC'))\\\n                                               .withColumn('order_timestamp_local', to_date('order_timestamp_local', 'YYYY-MM-DD'))\\\n                                               .withColumn('order_timestamp_local_month', month('order_timestamp_local'))\n# Orders and Sessions grouped by 'customer_id' and 'order_timestamp_local_month'\ngrouping_cols = ['customer_id','order_timestamp_local_month'] # 'payment_method', 'platform', 'device_model']\nsum_cols = ['order_total'\n            ,'credit'\n            ,'paid_amount'\n            ,'distance_merchant_customer'\n            ,'promo_is_promotion'\n            ,'normal_items_quantity'\n            ,'promo_items_quantity'\n            ,'session_duration_seconds'\n            ,'sum_event_open'\n            ,'sum_view_restaurant_screen'\n            ,'sum_view_dish_screen'\n            ,'sum_click_add_item'\n            ,'sum_view_checkout'\n            ,'sum_callback_purchase'\n            ,'order_session_quantity'\n            ,'valid_order'\n            ,'general_net_profit']\navg_cols = ['session_duration_seconds'\n            ,'distance_merchant_customer'\n            ,'general_net_profit'\n            ,'order_total'\n            ,'credit'\n            ,'paid_amount'\n            ,'sum_event_open'\n            ,'sum_view_restaurant_screen'\n            ,'sum_view_dish_screen'\n            ,'sum_click_add_item'\n            ,'sum_view_checkout'\n            ,'sum_callback_purchase']\n\n# Group Orders/Sessions base summing events of a customer in a month\ndf_orders_and_sessions_group = df_orders_and_sessions.groupBy(grouping_cols).agg(*[sum(c).alias(\"sum_\"+c) for c in sum_cols],\\\n                                                                                 *[avg(c).alias(\"avg_\"+c) for c in avg_cols],\\\n                                                                                 count('order_id').alias('number_of_orders'))\n\n# Most common categorical features on Orders/Sessions base\nimport pyspark.sql.functions as f\n@f.udf\ndef most_common_udf(x):\n    from collections import Counter\n    return Counter(x).most_common(1)[0][0]\n\ngrouping_cols = ['customer_id','order_timestamp_local_month']  \ncols = ['order_shift'\n        , 'delivery_type'\n        #, 'device_type'\n        , 'device_platform'\n        , 'payment_method'\n        , 'customer_state_label'\n        , 'customer_seg_recency_bucket'\n        , 'customer_seg_merchant_offer_bucket'\n        , 'customer_seg_benefits_sensitivity_bucket'\n        , 'customer_seg_frequency_bucket'\n        , 'customer_seg_gross_income_bucket'\n        , 'merchant_dish_type'\n       ]\nagg_expr = [most_common_udf(f.collect_list(col)).alias('most_common_'+col) for col in cols]\nmost_common_categorical = df_orders_and_sessions.groupBy(grouping_cols).agg(*agg_expr)\n\ncond = [df_orders_and_sessions_group.customer_id == most_common_categorical.customer_id, \n        df_orders_and_sessions_group.order_timestamp_local_month == most_common_categorical.order_timestamp_local_month]\ndf_orders_and_sessions_group = join_removing_repeated(df_orders_and_sessions_group, most_common_categorical, cond, 'left')\n\n\n# Unifies Pushes, Orders and Sessions user actions in a month\ncond = [df_orders_and_sessions_group.customer_id == df_customer_segmentation.customer_id, \n        df_orders_and_sessions_group.order_timestamp_local_month == df_customer_segmentation.segmentation_month_month]\ndf_join = join_removing_repeated(df_customer_segmentation, df_orders_and_sessions_group, cond, 'left')\n\ncond2 = [df_join.customer_id == df_mpf_grouped.external_user_id,\n        df_join.segmentation_month_month == df_mpf_grouped.event_month]\ndf_final = join_removing_repeated(df_join, df_mpf_grouped, cond2, how='left')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["# Saves table\n#df_final.write.saveAsTable('df_final_200609')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df_final = spark.table(\"df_final_200609\")\n\n# Cria a Target com 1 para cliente que foi Churn. 0 caso contrário\ndf_final = df_final.withColumn('target_current',when(df_final.ifood_status == 'Churn',1).otherwise(0))\n\ndf_final = df_final.withColumn('target',\n                  f.lag(df_final['target_current'])\n                   .over(Window.partitionBy(\"customer_id\")\n                   .orderBy(desc(\"segmentation_month_month\"))))\n\n\n# Para o lagging do problema 3, o churn/inativo do mês seguinte significa que o cliente nao fez nenhum pedido. Por exemplo, quem ficou inativo/churn tem sum_order_total null. Nesse caso, queremos prever que esse usuário vai ter um sum_order_total de zero.\ndf_final = df_final.fillna(0, subset=['sum_order_total'])\ndf_final = df_final.withColumn('target_3',\n                  f.lag(df_final['sum_order_total'])\n                   .over(Window.partitionBy(\"customer_id\")\n                   .orderBy(desc(\"segmentation_month_month\"))))\n\n# df_final = df_final.fillna(0, subset=['number_of_orders'])\n# df_final = df_final.withColumn('target_3',\n#                   f.lag(df_final['number_of_orders'])\n#                    .over(Window.partitionBy(\"customer_id\")\n#                    .orderBy(desc(\"segmentation_month_month\"))))\n\ndf_final = df_final.filter(~df_final.ifood_status.isin(['Churn', 'Inactive'])) # como o objetivo é prever churn, mantemos somente quem pode dar churn"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["# PREPARATION:\n# Listas de variaveis por tipo: categorico, numerico e data\ncategorical_columns = df_final.select(*[x[0] for x in df_final.dtypes if x[1] not in ('double', 'long', 'int', 'bigint', 'date')]).columns\nnumerical_columns = df_final.select(*[x[0] for x in df_final.dtypes if x[1] in ('double', 'long', 'int', 'bigint')]).columns\ndate_columns = df_final.select(*[x[0] for x in df_final.dtypes if x[1] in ('date')]).columns\n\n# Exclusão de colunas com dados referentes ao mês.\nexcluded_columns = [\n  'last_invalid_order_date'\n  ,'preferred_shift_bucket_description' # redundante pois preferred_shift_bucket foi tratado\n  ,'external_user_id' # chave da tabela de pushes\n  ,'event_month' # chave da tabela de pushes\n  ,'days_to_reorder_at_concluded' # variavel da tabela de segmentation que optamos por nao usar\n  ,'days_to_reorder_at_datasource' # variavel da tabela de segmentation que optamos por nao usar\n  ,'registration_month'\n  ,'registration_dayofweek'\n  ,'first_order_month'\n  ,'first_order_dayofweek'\n  ,'segmentation_month_dayofweek'\n  ,'segmentation_month' # redundante\n  ,'last_order_month'\n  ,'last_order_dayofweek'\n  ,'order_timestamp_local_month' # chave do join da base de order\n  ,'count_distinct_event_dayofweek' # nao tem uma interpretacao. Um 7 diz somente se um usuario recebeu num mes pushes todos os dias da semana\n  ,'event_month' # chave do join da base de pushes\n  ,'pushes_count_distinct_event_time_utc3' # timestamp da base de pushes\n  ,'sum_distance_merchant_customer' # from Orders+Visits: Nulo não faz sentido\n  ,'avg_distance_merchant_customer'\n  ,'most_common_merchant_dish_type'\n  ,'most_common_customer_state_label'\n  ,'most_common_platform'\n]\n\nincluded_categorical = [\n 'marlin_tag',\n 'last_nps',\n 'benefits_sensitivity_bucket',\n 'merchant_variety_bucket',\n 'most_common_order_shift',\n 'most_common_delivery_type',\n 'most_common_device_platform',\n 'most_common_payment_method',\n 'most_common_customer_seg_recency_bucket',\n 'most_common_customer_seg_merchant_offer_bucket',\n 'most_common_customer_seg_benefits_sensitivity_bucket',\n 'most_common_customer_seg_frequency_bucket',\n 'most_common_customer_seg_gross_income_bucket'\n]\n\nexcluded_columns = list(set(excluded_columns + date_columns + categorical_columns))\nincluded_columns = list(set(df_final.columns) - set(excluded_columns)) + ['customer_id'] + included_categorical\n\ndf_filtrado = df_final[included_columns]\n\n# Inputar zero para colunas numericas de pushes (clientes ativos que nao receberam push no mes) [~10050]\npushes_fillna_list = [\n  'pushes_changed_platform'\n  ,'pushes_count_distinct_event_date'\n  ,'pushes_count_distinct_campaign_name'\n]\nfor coluna in pushes_fillna_list:\n  df_filtrado = df_filtrado.fillna(0, subset=[coluna])\n\n# Drop em variaveis relacionadas a pedidos/sessões de clientes que nao fizeram compras (~3903)\ndf_filtrado = df_filtrado.dropna()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["# Temos uma base para modelagem com 110k registros\ndf_filtrado.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: 110882</div>"]}}],"execution_count":22},{"cell_type":"code","source":["df_filtrado.write.saveAsTable('df_filtrado_200610')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["## 6. [PROBLEMA 1] O que fizeram os clientes darem Churn?"],"metadata":{}},{"cell_type":"markdown","source":["### 6.1 Pipeline"],"metadata":{}},{"cell_type":"code","source":["included_categorical = [\n 'marlin_tag',\n 'last_nps',\n 'benefits_sensitivity_bucket',\n 'merchant_variety_bucket',\n 'most_common_order_shift',\n 'most_common_delivery_type',\n 'most_common_device_platform',\n 'most_common_payment_method',\n 'most_common_customer_seg_recency_bucket',\n 'most_common_customer_seg_merchant_offer_bucket',\n 'most_common_customer_seg_benefits_sensitivity_bucket',\n 'most_common_customer_seg_frequency_bucket',\n 'most_common_customer_seg_gross_income_bucket'\n]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["# PIPELINE BUILDING:\n# Based on https://gist.github.com/colbyford/83978917799dbcab6293521a60f29e94\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, OneHotEncoderEstimator, StringIndexer, VectorAssembler, MinMaxScaler, Normalizer\n\ndf_filtrado = spark.table(\"df_filtrado_200610\")\n\ncategoricalColumns = included_categorical\n\nnumericalColumns = list(set(df_filtrado.columns) - set(['customer_id','segmentation_month_month','ifood_status','ifood_status_last_month','target','target_3']) - set(categoricalColumns))\n\ncategoricalColumnsclassVec = [c + \"classVec\" for c in categoricalColumns]\n\nstages = []\n\nfor categoricalColumn in categoricalColumns:\n  print(categoricalColumn)\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalColumn, outputCol = categoricalColumn+\"Index\").setHandleInvalid(\"skip\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalColumn+\"Index\", outputCol=categoricalColumn+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]\n\n# Convert label into label indices using the StringIndexer\n#label_stringIndexer = StringIndexer(inputCol = label, outputCol = \"label\").setHandleInvalid(\"skip\")\n#stages += [label_stringIndexer]\n\n# Transform all features into a vector using VectorAssembler\nassemblerInputs = categoricalColumnsclassVec + numericalColumns \n# assembler only considers 'classVec' columns (it already did not consider stringIndexer)\nassembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")\n# assembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"featuresAssembled\")\nstages += [assembler]\n\nprepPipeline = Pipeline().setStages(stages)\npipelineModel = prepPipeline.fit(df_filtrado)\ndataset = pipelineModel.transform(df_filtrado)\n\n#dataset.write.saveAsTable('dataset_grupo_15')\n#dataset = spark.table(\"dataset_grupo_15\")\n\n# scaler = MinMaxScaler(inputCol=\"featuresAssembled\", outputCol=\"features\")\n# scalerModel = scaler.fit(dataset_assembled)\n# dataset = scalerModel.transform(dataset_assembled)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">marlin_tag\nlast_nps\nbenefits_sensitivity_bucket\nmerchant_variety_bucket\nmost_common_order_shift\nmost_common_delivery_type\nmost_common_device_platform\nmost_common_payment_method\nmost_common_customer_seg_recency_bucket\nmost_common_customer_seg_merchant_offer_bucket\nmost_common_customer_seg_benefits_sensitivity_bucket\nmost_common_customer_seg_frequency_bucket\nmost_common_customer_seg_gross_income_bucket\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["dataset.write.saveAsTable('dataset_churn_200610')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["###6.2 Modelagem"],"metadata":{}},{"cell_type":"markdown","source":["#### 6.2.1 Modelo 1: Regressão Logística"],"metadata":{}},{"cell_type":"code","source":["# TREINAMENTO E TESTE DO MODELO:\ndataset = spark.table(\"dataset_churn_200610\")\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\nreglog = LogisticRegression(labelCol='target')\n\nmodelo_rl = reglog.fit(treino)\npredicoes_rl = modelo_rl.transform(teste)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["#em caso de erro no import abaixo, instalar o pacote\n#!pip install handyspark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["# AUC ROC:\n# !pip install handyspark\nfrom handyspark import *\ntargetColumn = 'target'\n#predicoes_rl.toHandy().cols[['probability', 'prediction', targetColumn]][:5]\n\n# Creates instance of extended version of BinaryClassificationMetrics\n# using a DataFrame and its probability and label columns, as the output from the classifier\ntargetColumn = 'target'\nbcm_rl = BinaryClassificationMetrics(predicoes_rl, scoreCol='probability', labelCol=targetColumn)\n\n# We still can get the same metrics as the evaluator...\nprint(\"Area under ROC Curve: {:.4f}\".format(bcm_rl.areaUnderROC))\nprint(\"Area under PR Curve: {:.4f}\".format(bcm_rl.areaUnderPR))\n\n# But now we can PLOT both ROC and PR curves!\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nbcm_rl.plot_roc_curve(ax=axs[0])\nbcm_rl.plot_pr_curve(ax=axs[1])\n\n# # We can also get all metrics (FPR, Recall and Precision) by threshold\n# bcm.getMetricsByThreshold().filter('fpr between 0.19 and 0.21').toPandas().head(5)\n# # And get the confusion matrix for any threshold we want\n# bcm.print_confusion_matrix(.415856)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# MATRIZ DE CONFUSÃO:\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, roc_curve\nimport seaborn as sns\n\ny_true = predicoes_rl.select('target')\ny_true = y_true.toPandas()\n\ny_pred = predicoes_rl.select('prediction')\ny_pred = y_pred.toPandas()\ncm = confusion_matrix(y_true, y_pred)\n\nsns.set(font_scale=1.4)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}, fmt='g')"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# MÉTRICAS:\n\nprint('Acuracia Treino: ', modelo_rl.summary.accuracy)\nprint('Precision Treino: ', modelo_rl.summary.precisionByLabel)\nprint('Recall Treino: ', modelo_rl.summary.recallByLabel)\nprint('areaUnderRoc Treino: ', modelo_rl.summary.areaUnderROC)\n\nresultado_teste = modelo_rl.evaluate(teste)\n\nprint('Acuracia: ', resultado_teste.accuracy)\nprint('Precision: ', resultado_teste.precisionByLabel)\nprint('Recall: ', resultado_teste.recallByLabel)\nprint('areaUnderRoc: ', resultado_teste.areaUnderROC)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Acuracia Treino:  0.806630208039691\nPrecision Treino:  [0.8276891039071528, 0.6396496173983085]\nRecall Treino:  [0.9479509031150927, 0.31887767906439796]\nareaUnderRoc Treino:  0.8010095050318818\nAcuracia:  0.803802315628238\nPrecision:  [0.8242886178861789, 0.6436233611442194]\nRecall:  [0.947602079560722, 0.31902323749507683]\nareaUnderRoc:  0.8061284213940544\n</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["#### 6.2.2 Modelo 2: Decision Tree"],"metadata":{}},{"cell_type":"code","source":["# TREINAMENTO E TESTE DO MODELO:\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\ndecTree = DecisionTreeClassifier(labelCol='target')\n\nmodelo_dt = decTree.fit(treino)\n\npredicoes_dt = modelo_dt.transform(teste)\nresultado_dt = predicoes_dt.select(\"target\", \"prediction\", \"probability\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37},{"cell_type":"code","source":["ExtractFeatureImp(modelo_dt.featureImportances, predicoes_dt, \"features\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>name</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>56</td>\n      <td>qtt_orders_last_year</td>\n      <td>0.901243</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>79</td>\n      <td>number_of_orders</td>\n      <td>0.035849</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>1</td>\n      <td>marlin_tagclassVec_2. Tilapia</td>\n      <td>0.021835</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>58</td>\n      <td>qtt_valid_orders</td>\n      <td>0.019388</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>49</td>\n      <td>recency_days</td>\n      <td>0.012742</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>62</td>\n      <td>customer_lifetime_months</td>\n      <td>0.004361</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>3</td>\n      <td>last_npsclassVec_Sem Avaliacoes</td>\n      <td>0.003663</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>51</td>\n      <td>pushes_count_distinct_event_date</td>\n      <td>0.000919</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>46</td>\n      <td>avg_order_total</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>14</td>\n      <td>most_common_order_shiftclassVec_weekend lunch</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>15</td>\n      <td>most_common_order_shiftclassVec_weekday snack</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>16</td>\n      <td>most_common_order_shiftclassVec_weekend dawn</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>18</td>\n      <td>most_common_order_shiftclassVec_weekday dawn</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>17</td>\n      <td>most_common_order_shiftclassVec_weekend snack</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>12</td>\n      <td>most_common_order_shiftclassVec_weekday dinner</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>19</td>\n      <td>most_common_order_shiftclassVec_weekday breakfast</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>20</td>\n      <td>most_common_delivery_typeclassVec_DELIVERY</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>13</td>\n      <td>most_common_order_shiftclassVec_weekday lunch</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>9</td>\n      <td>merchant_variety_bucketclassVec_Alta</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>11</td>\n      <td>most_common_order_shiftclassVec_weekend dinner</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>10</td>\n      <td>merchant_variety_bucketclassVec_Pedido Unico</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>22</td>\n      <td>most_common_device_platformclassVec_IOS</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>8</td>\n      <td>merchant_variety_bucketclassVec_Media</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>7</td>\n      <td>benefits_sensitivity_bucketclassVec_Media</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>6</td>\n      <td>benefits_sensitivity_bucketclassVec_Alta</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>5</td>\n      <td>last_npsclassVec_Neutral</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>4</td>\n      <td>last_npsclassVec_Promoter</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>2</td>\n      <td>marlin_tagclassVec_4. Retention Carp</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>21</td>\n      <td>most_common_device_platformclassVec_ANDROID</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>25</td>\n      <td>most_common_payment_methodclassVec_OTHERS_OFFLINE</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>54</td>\n      <td>sum_sum_view_restaurant_screen</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>53</td>\n      <td>orders_last_91d</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>52</td>\n      <td>recency_days_bucket</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50</td>\n      <td>sum_normal_items_quantity</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>48</td>\n      <td>avg_sum_event_open</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>73</td>\n      <td>sum_general_net_profit</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>75</td>\n      <td>top_dish_bucket</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>98</td>\n      <td>merchant_variety</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>88</td>\n      <td>sum_sum_view_dish_screen</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>97</td>\n      <td>avg_sum_click_add_item</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>47</td>\n      <td>benefits_sensitivity</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>95</td>\n      <td>avg_session_duration_seconds</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>94</td>\n      <td>sum_order_total</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>93</td>\n      <td>sum_paid_amount</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>92</td>\n      <td>rfv_score</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>91</td>\n      <td>avg_sum_view_restaurant_screen</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>90</td>\n      <td>freq_last_91d_bucket</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>89</td>\n      <td>avg_credit</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>87</td>\n      <td>target_current</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>76</td>\n      <td>merchant_offer</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>86</td>\n      <td>customer_lifetime_days</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>85</td>\n      <td>sum_order_session_quantity</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>84</td>\n      <td>freq_last_91d</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>83</td>\n      <td>qtt_invalid_orders</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>82</td>\n      <td>maturity_orders_bucket</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>81</td>\n      <td>sum_valid_order</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>80</td>\n      <td>preferred_shift_bucket</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>78</td>\n      <td>avg_sum_view_dish_screen</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>77</td>\n      <td>avg_sum_view_checkout</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>45</td>\n      <td>most_common_customer_seg_gross_income_bucketcl...</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>101 rows × 3 columns</p>\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["# AUC ROC:\n\n# Creates instance of extended version of BinaryClassificationMetrics\n# using a DataFrame and its probability and label columns, as the output from the classifier\ntargetColumn = 'target'\nbcm_dt = BinaryClassificationMetrics(predicoes_dt, scoreCol='probability', labelCol=targetColumn)\n\n# But now we can PLOT both ROC and PR curves!\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nbcm_dt.plot_roc_curve(ax=axs[0])\nbcm_dt.plot_pr_curve(ax=axs[1])"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# MATRIZ DE CONFUSÃO:\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, roc_curve\nimport seaborn as sns\n\ny_trueDT = predicoes_dt.select('target')\ny_trueDT = y_trueDT.toPandas()\n\ny_predDT = predicoes_dt.select('prediction')\ny_predDT = y_predDT.toPandas()\ncm = confusion_matrix(y_trueDT, y_predDT)\n\nsns.set(font_scale=1.4)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}, fmt='g')"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# MÉTRICAS:\n\nevaluator = BinaryClassificationEvaluator(labelCol='target', metricName='areaUnderROC')\nclass_model_evaluator(predicoes_dt, labelCol='target')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">AUC ROC: 0.30986615803224926\nAccuracy: 0.8011442987791143\nPrecision: 0.7791141233897686\nRecall: 0.8011442987791143\nF1-score: 0.7729056308188648\n</div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["#### 6.2.3 Modelo 3 - Random Forest"],"metadata":{}},{"cell_type":"code","source":["# TREINAMENTO E TESTE DO MODELO:\n\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\nrf = RandomForestClassifier(labelCol='target', featuresCol=\"features\", numTrees=10)\n\nmodelo_rf = rf.fit(treino)\n\npredicoes_rf = modelo_rf.transform(teste)\nresultado_rf = predicoes_rf.select(\"target\", \"prediction\", \"probability\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":43},{"cell_type":"code","source":["# AUC ROC:\n\n# Creates instance of extended version of BinaryClassificationMetrics\n# using a DataFrame and its probability and label columns, as the output from the classifier\ntargetColumn = 'target'\nbcm_rf = BinaryClassificationMetrics(predicoes_rf, scoreCol='probability', labelCol=targetColumn)\n\n# But now we can PLOT both ROC and PR curves!\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nbcm_rf.plot_roc_curve(ax=axs[0])\nbcm_rf.plot_pr_curve(ax=axs[1])"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# MATRIZ DE CONFUSÃO:\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, roc_curve\nimport seaborn as sns\n\ny_trueRF = predicoes_rf.select('target')\ny_trueRF = y_trueRF.toPandas()\n\ny_predRF = predicoes_rf.select('prediction')\ny_predRF = y_predRF.toPandas()\ncm = confusion_matrix(y_trueRF, y_predRF)\n\nsns.set(font_scale=1.4)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}, fmt='g')"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# MÉTRICAS:\n\nclass_model_evaluator(predicoes_rf, labelCol='target')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">AUC ROC: 0.7960405040443785\nAccuracy: 0.7997477136549984\nPrecision: 0.781017244075669\nRecall: 0.7997477136549984\nF1-score: 0.7588583939387468\n</div>"]}}],"execution_count":46},{"cell_type":"markdown","source":["#### 6.2.4 Modelo 4 - Gradient Boosted Tree"],"metadata":{}},{"cell_type":"code","source":["# TREINAMENTO E TESTE DO MODELO:\n\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\ngbt = GBTClassifier(labelCol=\"target\", featuresCol=\"features\", maxIter=10)\nmodelo_gbt = gbt.fit(treino)\npredicoes_gbt = modelo_gbt.transform(teste)\nresultado_gbt = predicoes_gbt.select(\"target\", \"prediction\", \"probability\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":48},{"cell_type":"code","source":["# AUC ROC:\n\n# Creates instance of extended version of BinaryClassificationMetrics\n# using a DataFrame and its probability and label columns, as the output from the classifier\ntargetColumn = 'target'\nbcm_gbt = BinaryClassificationMetrics(predicoes_gbt, scoreCol='probability', labelCol=targetColumn)\n\n# But now we can PLOT both ROC and PR curves!\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nbcm_gbt.plot_roc_curve(ax=axs[0])\nbcm_gbt.plot_pr_curve(ax=axs[1])"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["# MATRIZ DE CONFUSÃO:\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, roc_curve\nimport seaborn as sns\n\ny_trueGB = predicoes_gbt.select('target')\ny_trueGB = y_trueGB.toPandas()\n\ny_predGB = predicoes_gbt.select('prediction')\ny_predGB = y_predGB.toPandas()\ncm = confusion_matrix(y_trueGB, y_predGB)\n\nsns.set(font_scale=1.4)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}, fmt='g')"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# MÉTRICAS:\n\nclass_model_evaluator(predicoes_gbt, labelCol='target')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">AUC ROC: 0.8048043488458954\nAccuracy: 0.804523133756814\nPrecision: 0.7840132813666125\nRecall: 0.8045231337568141\nF1-score: 0.778072533869005\n</div>"]}}],"execution_count":51},{"cell_type":"markdown","source":["#### 6.2.5 Modelo 5 - Naive Bayes"],"metadata":{}},{"cell_type":"code","source":["# TREINAMENTO E TESTE DO MODELO:\n\n# Reference: http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.feature import MinMaxScaler\n\nscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"features_scaled\")\nscalerModel = scaler.fit(dataset)\nscalerData = scalerModel.transform(dataset)\ntreino, teste = scalerData.randomSplit([0.8, 0.2], seed = 42)\n\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol='features_scaled', labelCol='target')\nmodelo_nb = nb.fit(treino)\npredicoes_nb = modelo_nb.transform(teste)\nresultado_nb = predicoes_nb.select(\"target\", \"prediction\", \"probability\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":53},{"cell_type":"code","source":["# AUC ROC:\n\n# Creates instance of extended version of BinaryClassificationMetrics\n# using a DataFrame and its probability and label columns, as the output from the classifier\ntargetColumn = 'target'\nbcm_nb = BinaryClassificationMetrics(predicoes_nb, scoreCol='probability', labelCol=targetColumn)\n\n# But now we can PLOT both ROC and PR curves!\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nbcm_nb.plot_roc_curve(ax=axs[0])\nbcm_nb.plot_pr_curve(ax=axs[1])"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# MATRIZ DE CONFUSÃO:"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# MÉTRICAS:\n\nclass_model_evaluator(predicoes_nb, labelCol='target')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">AUC ROC: 0.6716840398608138\nAccuracy: 0.7528494841645267\nPrecision: 0.7716212199607154\nRecall: 0.7528494841645268\nF1-score: 0.7605433907542607\n</div>"]}}],"execution_count":56},{"cell_type":"markdown","source":["### 6.3 Comparação dos Modelos"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\n\n\nplt.plot(modelo_rl.summary.roc.select('FPR').collect(),\n         modelo_rl.summary.roc.select('TPR').collect(), label='Regressão Logistica')\n\n# Decision Tree\npreds_dt = predicoes_dt.select('target','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['target'])))\nfpr_dt, tpr_dt = CurveMetrics(preds_dt).get_curve('roc')\nplt.plot(fpr_dt, tpr_dt, label='Árvore Decisão')\n\n# Random Forest\npreds_rf = predicoes_rf.select('target','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['target'])))\nfpr_rf, tpr_rf = CurveMetrics(preds_rf).get_curve('roc')\nplt.plot(fpr_rf, tpr_rf, label='Random Forest')\n\n# Gradient Boosting Tree\npreds_gbt = predicoes_gbt.select('target','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['target'])))\nfpr_gbt, tpr_gbt = CurveMetrics(preds_gbt).get_curve('roc')\nplt.plot(fpr_gbt, tpr_gbt, label='Gradient Boosting')\n\n# Naive Bayes\npreds_nb = predicoes_nb.select('target','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['target'])))\nfpr_nb, tpr_nb = CurveMetrics(preds_nb).get_curve('roc')\nplt.plot(fpr_nb, tpr_nb, label='Naive Bayes')\n\nplt.title('ROC')\nplt.legend(loc=\"best\")\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["print(\"Area under ROC: \")\nprint(\"Logistic Regression   - Area under ROC Curve: {:.4f}\".format(bcm_rl.areaUnderROC))\nprint(\"Decision Tree         - Area under ROC Curve: {:.4f}\".format(bcm_dt.areaUnderROC))\nprint(\"Random Forest         - Area under ROC Curve: {:.4f}\".format(bcm_rf.areaUnderROC))\nprint(\"Gradient Boosting Tree - Area under ROC Curve: {:.4f}\".format(bcm_gbt.areaUnderROC))\nprint(\"Naive Bayes           - Area under ROC Curve: {:.4f}\".format(bcm_nb.areaUnderROC))\nprint(\"\")\nprint(\"Area under PR: \")\nprint(\"Logistic Regression   - Area under PR Curve: {:.4f}\".format(bcm_rl.areaUnderPR))\nprint(\"Decision Tree         - Area under PR Curve: {:.4f}\".format(bcm_dt.areaUnderPR))\nprint(\"Random Forest         - Area under PR Curve: {:.4f}\".format(bcm_rf.areaUnderPR))\nprint(\"Gradient Boosting Tree - Area under PR Curve: {:.4f}\".format(bcm_gbt.areaUnderPR))\nprint(\"Naive Bayes           - Area under PR Curve: {:.4f}\".format(bcm_nb.areaUnderPR))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Area under ROC: \nLogistic Regression   - Area under ROC Curve: 0.8062\nDecision Tree         - Area under ROC Curve: 0.7075\nRandom Forest         - Area under ROC Curve: 0.7960\nGradient Boosted Tree - Area under ROC Curve: 0.8048\nNaive Bayes           - Area under ROC Curve: 0.7772\n\nArea under PR: \nLogistic Regression   - Area under PR Curve: 0.5676\nDecision Tree         - Area under PR Curve: 0.5336\nRandom Forest         - Area under PR Curve: 0.5570\nGradient Boosted Tree - Area under PR Curve: 0.5743\nNaive Bayes           - Area under PR Curve: 0.4909\n</div>"]}}],"execution_count":59},{"cell_type":"markdown","source":["**Resultados:** Portanto, baseando-se na *AUC ROC* vê-se que a **Regressão Logística** e o **Gradiente Boosting Tree** foram os modelos com melhores desempenhos (0.8062 e 0.8048 respectivamente). O modelo com o pior desempenho foi a árvore de decisão (0.7075) e, inclusive, salta aos olhos sua curva no gráfico comparativo recentemente plotado (Cmd 59). Em termos de *Acurácia*, **Regressão Logística** e **Gradiente Boosting Tree** também foram os modelos com o melhor desempenho (0.8038 e 0,8045 respectivamente).\n\nCom isso, a seguir aplicaremos técnicas de seleção de variáveis, otimização dos hiperparâmetros e validação cruzada nos dois modelos selecionados na expectativa de melhorar ainda mais os resultados obtidos."],"metadata":{}},{"cell_type":"markdown","source":["### 6.4 Feature Selection using Random Forest"],"metadata":{}},{"cell_type":"code","source":["# Reference: http://people.stat.sc.edu/haigang/improvement.html\n# https://www.timlrx.com/2018/06/19/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator/\n# Random Forest\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ndataset = spark.table(\"dataset_churn_200610\")\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\nrf = RandomForestClassifier(labelCol='target', featuresCol=\"features\", numTrees=10)\n\nmodelo_rf = rf.fit(treino)\n\npredicoes_rf = modelo_rf.transform(teste)\nresultado_rf = predicoes_rf.select(\"target\", \"prediction\", \"probability\")\n\n#class_model_evaluator(predicoes_rf,labelCol='target')\n\nExtractFeatureImp(modelo_rf.featureImportances, predicoes_rf, \"features\").head(10)\nvarlist = ExtractFeatureImp(modelo_rf.featureImportances, predicoes_rf, \"features\")\nvaridx = [x for x in varlist['idx'][0:75]]\n\nvarlist[varlist['score'] > 0].count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: idx      39\nname     39\nscore    39\ndtype: int64</div>"]}}],"execution_count":62},{"cell_type":"code","source":["# Plot feature importances (Para slide)\nimport seaborn as sns\n\nfig=plt.figure(figsize=[10,5])\nax=fig.add_subplot(111)\nax.set_title(\"Top 10 Features Random Forest\")\nax = sns.barplot(x='score', y='name',data=varlist.head(10), color=(0.2, 0.4, 0.6, 0.6))\nplt.xticks(rotation=0) \nplt.show()"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["### 6.5 Hyperparameters and Cross Validation"],"metadata":{}},{"cell_type":"markdown","source":["#### 6.5.1 Logistic Regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorSlicer\n#import mlflow\n\ndataset_renamed = dataset.withColumnRenamed('target','label')\ntreino, teste = dataset_renamed.randomSplit([0.8, 0.2], seed = 42)\n\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\nmodeloRegLog = LogisticRegression(featuresCol='features_subset', labelCol='label')\n\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(modeloRegLog.regParam, [0.1, 0.01])\\\n    .addGrid(modeloRegLog.maxIter, [30, 60])\\\n    .build()\n\nevaluator = BinaryClassificationEvaluator()\n\ncrossval = CrossValidator(estimator=modeloRegLog,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\n\n# Deve demorar ~16 min\nimport time\nti = time.time()\ncvModelo = crossval.fit(treino_subset)\ntf = time.time()\nprint(\"Demorou {} segundos\".format(tf - ti))\n\nmelhorModelo = cvModelo.bestModel\nprint(\"Max Iter - \", melhorModelo._java_obj.parent().getMaxIter())\nprint(\"Reg Param - \", melhorModelo._java_obj.parent().getRegParam())\n\ncvPrevisoes_RL = melhorModelo.transform(teste_subset)\nprint(\"areaUnderROC:\", evaluator.setMetricName(\"areaUnderROC\").evaluate(cvPrevisoes_RL))\nprint(\"areaUnderPR:\", evaluator.setMetricName(\"areaUnderPR\").evaluate(cvPrevisoes_RL))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\nDemorou 880.0643780231476 segundos\nMax Iter -  60\nReg Param -  0.01\nareaUnderROC: 0.8025008247413756\nareaUnderPR: 0.5602960006054394\n</div>"]}}],"execution_count":66},{"cell_type":"code","source":["# Removendo versao antiga do modelo salvo\nmodelpath = \"/dbfs/FileStore/models/model_problema1_cv_lr\"\ndbutils.fs.rm(modelpath, True)\nmodelpath = \"/dbfs/FileStore/models/model_problema1_cv_lr\"\nmelhorModelo.write().overwrite().save(modelpath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":67},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegressionModel\ndataset = spark.table(\"dataset_churn_200610\").repartition(2).cache()\ntarget = 'target'\ndataset_renamed = dataset.withColumnRenamed('target','label')\ntreino, teste = dataset_renamed.randomSplit([0.8, 0.2], seed = 42)\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nmodelpath = \"/dbfs/FileStore/models/model_problema1_cv_lr\"\nsaved_melhorModelo_lr = LogisticRegressionModel.load(modelpath)\ncv_predicoes_lr = saved_melhorModelo_lr.transform(teste_subset)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":68},{"cell_type":"code","source":["# MATRIZ DE CONFUSÃO:\n# rodei no notebook cópia dessa entrega em: https://community.cloud.databricks.com/?o=763780990988853#notebook/2051424721805341/command/1208499454968453\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, roc_curve\nimport seaborn as sns\n\ny_true = cvPrevisoes_RL.select('label')\ny_true = y_true.toPandas()\n\ny_pred = cvPrevisoes_RL.select('prediction')\ny_pred = y_pred.toPandas()\ncm = confusion_matrix(y_true, y_pred)\n\nsns.set(font_scale=1.4)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}, fmt='g')\n\nprint('Acurácia:', accuracy_score(y_true, y_pred))\nprint('Acurácia Balanceada:', balanced_accuracy_score(y_true, y_pred))\nprint(classification_report(y_true, y_pred))"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegressionModel\ndataset = spark.table(\"dataset_churn_200610\").repartition(2).cache()\ntarget = 'target'\ndataset_renamed = dataset.withColumnRenamed('target','label')\ntreino, teste = dataset_renamed.randomSplit([0.8, 0.2], seed = 42)\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nmodelpath = \"/dbfs/FileStore/models/model_problema1_cv_lr\"\nsaved_melhorModelo_lr = LogisticRegressionModel.load(modelpath)\ncv_predicoes_lr = saved_melhorModelo_lr.transform(teste_subset)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":70},{"cell_type":"markdown","source":["#### 6.5.2 Gradient Boosting Tree"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.classification import GBTClassifier\nimport time\n\ndataset_renamed = dataset.withColumnRenamed('target','label')\ntreino, teste = dataset_renamed.randomSplit([0.8, 0.2], seed = 42)\n\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\ngbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features_subset\", maxIter=10)\n\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [6])\n             .addGrid(gbt.maxBins, [30])\n             .addGrid(gbt.maxIter, [15])\n             .build())\n\n# paramGrid = (ParamGridBuilder()\n#              .addGrid(gbt.maxDepth, [2, 4, 6])\n#              .addGrid(gbt.maxBins, [20, 30])\n#              .addGrid(gbt.maxIter, [10, 15])\n#              .build())\n\nevaluator = BinaryClassificationEvaluator()\n\ncv_gb = CrossValidator(estimator=gbt\n                       ,estimatorParamMaps=paramGrid\n                       ,evaluator=evaluator\n                       ,numFolds=5)\n\nti = time.time()\ncvModelo_gb = cv_gb.fit(treino_subset)\ntf = time.time()\nprint(\"Demorou {} segundos\".format(tf - ti))\n\nmelhorModelo_gb = cvModelo_gb.bestModel\nprint(\"MaxDepth - \", melhorModelo_gb._java_obj.getMaxDepth())\nprint(\"MaxBins - \", melhorModelo_gb._java_obj.getMaxBins())\nprint(\"MaxBins - \", melhorModelo_gb._java_obj.getMaxIter())\n\ncvPrevisoes_gb = melhorModelo_gb.transform(teste_subset)\nprint(\"areaUnderROC:\", evaluator.setMetricName(\"areaUnderROC\").evaluate(cvPrevisoes_gb))\nprint(\"areaUnderPR:\", evaluator.setMetricName(\"areaUnderPR\").evaluate(cvPrevisoes_gb))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\nDemorou 982.3825798034668 segundos\nMaxDepth -  6\nMaxBins -  30\nMaxBins -  15\nareaUnderROC: 0.8074146877839409\nareaUnderPR: 0.5796069667454931\n</div>"]}}],"execution_count":72},{"cell_type":"code","source":["# Removendo versao antiga do modelo salvo\nmodelpath = \"/dbfs/FileStore/models/model_problema1_cv_gbt\"\ndbutils.fs.rm(modelpath, True)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["modelpath = \"/dbfs/FileStore/models/model_problema1_cv_gbt\"\nmelhorModelo_gb.write().overwrite().save(modelpath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":74},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier, GBTClassificationModel\ndataset = spark.table(\"dataset_churn_200610\").repartition(2).cache()\ntarget = 'target'\ndataset_renamed = dataset.withColumnRenamed('target','label')\ntreino, teste = dataset_renamed.randomSplit([0.8, 0.2], seed = 42)\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nmodelpath = \"/dbfs/FileStore/models/model_problema1_cv_gbt\"\nsaved_melhorModelo_gbt = GBTClassificationModel.load(modelpath)\ncv_predicoes_gbt = saved_melhorModelo_gbt.transform(teste_subset)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":75},{"cell_type":"code","source":["# https://stackoverflow.com/questions/42549200/how-to-get-all-parameters-of-estimator-in-pyspark\n{param[0].name: param[1] for param in saved_melhorModelo_gbt.extractParamMap().items()}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[21]: {&#39;cacheNodeIds&#39;: False,\n &#39;checkpointInterval&#39;: 10,\n &#39;featureSubsetStrategy&#39;: &#39;all&#39;,\n &#39;impurity&#39;: &#39;gini&#39;,\n &#39;lossType&#39;: &#39;logistic&#39;,\n &#39;maxMemoryInMB&#39;: 256,\n &#39;minInfoGain&#39;: 0.0,\n &#39;minInstancesPerNode&#39;: 1,\n &#39;predictionCol&#39;: &#39;prediction&#39;,\n &#39;probabilityCol&#39;: &#39;probability&#39;,\n &#39;rawPredictionCol&#39;: &#39;rawPrediction&#39;,\n &#39;seed&#39;: 3504127614838123891,\n &#39;stepSize&#39;: 0.1,\n &#39;subsamplingRate&#39;: 1.0,\n &#39;validationTol&#39;: 0.01,\n &#39;featuresCol&#39;: &#39;features_subset&#39;,\n &#39;labelCol&#39;: &#39;label&#39;,\n &#39;maxBins&#39;: 30,\n &#39;maxDepth&#39;: 6,\n &#39;maxIter&#39;: 15}</div>"]}}],"execution_count":76},{"cell_type":"code","source":["# Plot feature importances (Para slide)\nimport seaborn as sns\nvarlist = ExtractFeatureImp(saved_melhorModelo_gbt.featureImportances, cv_predicoes_gbt, \"features_subset\")\n\nfig=plt.figure(figsize=[10,5])\nax=fig.add_subplot(111)\nax.set_title(\"Top 10 Features Random Forest\")\nax = sns.barplot(x='score', y='name',data=varlist.head(15), color=(0.2, 0.4, 0.6, 0.6))\nplt.xticks(rotation=0) \nplt.show()"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["# AUC ROC:\n\n# Creates instance of extended version of BinaryClassificationMetrics\n# using a DataFrame and its probability and label columns, as the output from the classifier\ntargetColumn = 'label'\nbcm_gbt_cv = BinaryClassificationMetrics(cvPrevisoes_gb, scoreCol='probability', labelCol=targetColumn)\n\n# But now we can PLOT both ROC and PR curves!\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nbcm_gbt_cv.plot_roc_curve(ax=axs[0])\nbcm_gbt_cv.plot_pr_curve(ax=axs[1])"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["# MATRIZ DE CONFUSÃO:\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, roc_curve\nimport seaborn as sns\n\ny_true = cvPrevisoes_gb.select('label')\ny_true = y_true.toPandas()\n\ny_pred = cvPrevisoes_gb.select('prediction')\ny_pred = y_pred.toPandas()\ncm = confusion_matrix(y_true, y_pred)\n\nsns.set(font_scale=1.4)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}, fmt='g')\n\nprint('Acurácia:', accuracy_score(y_true, y_pred))\nprint('Acurácia Balanceada:', balanced_accuracy_score(y_true, y_pred))\nprint(classification_report(y_true, y_pred))"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["#### 6.5.3 Comparação Resultados"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\n\nplt.plot(melhorModelo.summary.roc.select('FPR').collect(),\n         melhorModelo.summary.roc.select('TPR').collect(), label='Regressão Logistica')\n\n# Gradient Boosting Tree\nlabelColumn = 'label' # pois renomeei a 'target' para 'label' para o cv funcionar\ncv_preds_gbt = cvPrevisoes_gb.select(labelColumn,'probability').rdd.map(lambda row: (float(row['probability'][1]), float(row[labelColumn])))\nfpr_gbt_cv, tpr_gbt_cv = CurveMetrics(cv_preds_gbt).get_curve('roc')\nplt.plot(fpr_gbt_cv, tpr_gbt_cv, label='Gradient Boosting')\n\nplt.title('ROC')\nplt.legend(loc=\"best\")\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["print(\"Area under ROC: \")\nprint(\"Logistic Regression CV   - Area under ROC Curve: {:.4f}\".format(bcm_rl_cv.areaUnderROC))\nprint(\"Gradient Boosted Tree CV - Area under ROC Curve: {:.4f}\".format(bcm_gbt_cv.areaUnderROC))\nprint(\"\")\nprint(\"Area under PR: \")\nprint(\"Logistic Regression CV   - Area under PR Curve: {:.4f}\".format(bcm_rl_cv.areaUnderPR))\nprint(\"Gradient Boosted Tree CV - Area under PR Curve: {:.4f}\".format(bcm_gbt_cv.areaUnderPR))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Area under ROC: \nLogistic Regression CV   - Area under ROC Curve: 0.8025\nGradient Boosted Tree CV - Area under ROC Curve: 0.8074\n\nArea under PR: \nLogistic Regression CV   - Area under PR Curve: 0.5603\nGradient Boosted Tree CV - Area under PR Curve: 0.5796\n</div>"]}}],"execution_count":82},{"cell_type":"markdown","source":["### 6.6 Conclusão e Resposta do Problema"],"metadata":{}},{"cell_type":"markdown","source":["**Resultados:** Por fim, os dois modelos tiveram desempenhos bastante parecidos, com ligeira vantagem para o **Gradiente Boosting Tree**:\n- GBT: AUC ROC = 0.8074.\n- Regressão Logística: AUC ROC = 0.8025.\n\nA otimização também gerou em uma melhoria mínima de 0.0026. Isso se deu principalmente por conta de limitações dos recursos computacionais disponíveis. Entretanto, dado que o objetivo principal dessa abordagem foi identificar as variáveis mais importantes para o modelo para responder a pergunta do problema, o obtido foi suficiente.\n  \n**Conclusão e Resposta para o problema inicial:**\n\nRecaptulando, para responder a pergunta do que fez os clientes darem churn, decidiu-se criar um modelo para prever o churn e então usar essencialmente as *features importances* do melhor modelo para como os fatores que mais influenciaram no churn. Com o modelo criado e otimizado, o próximo *Cmd* trás então um plot das 15 features mais importantes para o modelo GBT. No nosso caso, esses então são os principais fatores que influenciaram para os clientes darem churn.\n\nIndo mais a fundo nos resultado, pode-se ver que o modelo identificou como principais variáveis *orders_last_91d, recency_days* e *qtt_orders_last_year*, e isso faz total sentido dado que elas dão contexto de sazonalidade. Por exemplo, como *recency_days* é a diferença em dias entre hoje e a última compra do usuário, faz sentido pensar que se alguém tem poucas compras nos últimos 91 dias o indivíduo demonstra estar menos engajado com a plataforma do que alguém com muitos pedidos nesse mesmo período. Nesse caso, além de intuitivamente, vê-se que esse tipo de informação é um indicativo forte para alguém que vai dar churn.\n\nEm seguida, vemos que as variáveis quantitativas relacionadas ao comportamento de compra no mês (*sum_order_total, sum_paid_amount*) também são relevantes para prever se um cliente vira churn no mês seguinte.\n\nÉ interessante notar também que variáveis qualitativas são bem relevantes para o modelo, tais como a *marlin_tagclassVec_4. Retention Carp*."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier, GBTClassificationModel\ndataset = spark.table(\"dataset_churn_200610\").repartition(2).cache()\ntarget = 'target'\ndataset_renamed = dataset.withColumnRenamed('target','label')\ntreino, teste = dataset_renamed.randomSplit([0.8, 0.2], seed = 42)\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nmodelpath = \"/dbfs/FileStore/models/model_problema1_cv_gbt\"\nsaved_melhorModelo_gbt = GBTClassificationModel.load(modelpath)\ncv_predicoes_gbt = saved_melhorModelo_gbt.transform(teste_subset)\n# Plot feature importances (Para slide)\nimport seaborn as sns\nvarlist = ExtractFeatureImp(saved_melhorModelo_gbt.featureImportances, cv_predicoes_gbt, \"features_subset\")\n\nfig=plt.figure(figsize=[10,5])\nax=fig.add_subplot(111)\nax.set_title(\"Top Features Gradient Boosted Tree\")\nax = sns.barplot(x='score', y='name',data=varlist.head(15), color=(0.2, 0.4, 0.6, 0.6))\nplt.xticks(rotation=0) \nplt.show()"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["## 7. [PROBLEMA 2] Qual a importância dos eventos ao longo da vida do cliente?"],"metadata":{}},{"cell_type":"markdown","source":["**Idéia:** Dado que o conceito de importância não foi previamente estabelecido, decidimos então inicialmente explorar a relação entre os eventos ao longo do tempo e, partindo daí, tentar responder algumas questões levantadas na etapada de exploração (nessa a seguir e algumas até mesmo na Entrega 1). Tais exploração e questões serão abordados ao longo do código.\n  \n**Definições e Assumptions:**\n- *Importância*: partindo do pressuposto de que o Ifood deseja que seus clientes permaneçam ativos e comprando e gastando cada vez mais, define-se como a importância de um evento o quanto ele demonstra ter colaborado para que o Ifood atinja seus objetivos citados com seus clientes (individual ou coletivamente) ao longo de suas vidas.\n\n**Observações:**\n- É sabido que só se tem informações de visitas/acessos que geraram pedidos. Isso faz com que a relação entre visitas e pedidos seja proporcional e, dado o conceito tomado de importância, impossibilita argumentações muito mais específicas."],"metadata":{}},{"cell_type":"markdown","source":["### 7.1 Attribution Window"],"metadata":{}},{"cell_type":"markdown","source":["Como primeiro ponto de partida, queremos saber o quanto um push influencia em uma compra e o quanto isso varia sob um olhar de diferentes segmentos da base do Ifood. </br></br>\nAssim, abaixo montamos uma tabela que associa para todo push recebido a um pedido/sessão em uma janela de até 30min (1800s). Por exemplo, se um push foi enviado as 12h toda sessão que se iniciou até 30min após o horário de envio do push (até 12h30) vai ser atribuido ao push. </br> </br>\nIdealmente seria necessário ter a base de sessões que não geraram pedidos para termos uma visão mais completa sobre o quanto o excesso de pushes causa de percepção negativa no usuário."],"metadata":{}},{"cell_type":"code","source":["# Read from Parquet\nroot_dir = '/dbfs/FileStore/treated'\ndbutils.fs.ls(f'{root_dir}')\ndf_customer_segmentation = spark.read.parquet(f'{root_dir}/df_customer_segmentation.parquet')\ndf_orders_total_tratado = spark.read.parquet(f'{root_dir}/df_orders_total.parquet')\ndf_sessions_visits_tratado = spark.read.parquet(f'{root_dir}/df_sessions_visits.parquet')\ndf_mpf_tratado = spark.read.parquet(f'{root_dir}/df_mpf.parquet')\n\npushes_received = df_mpf_tratado[df_mpf_tratado['event_name'] == 'received'].select('external_user_id', 'event_time_utc3')\ndf_orders_and_sessions = join_removing_repeated(df_orders_total_tratado, df_sessions_visits_tratado, \n                                                df_orders_total_tratado.session_id == df_sessions_visits_tratado.session_id, 'left')\napp_sessions = df_orders_and_sessions.withColumn('order_timestamp_local', from_utc_timestamp('order_timestamp_local', 'UTC'))\\\n                                     .withColumn('order_timestamp_local', to_date('order_timestamp_local', 'YYYY-MM-DD'))\\\n                                     .withColumn('order_timestamp_local_month', month('order_timestamp_local'))\\\n                                     .select('customer_id'\n                                             ,'order_timestamp_local_month'\n                                             ,'session_started_at_amsp'\n                                             ,'session_ended_at_amsp'\n                                             ,'session_started_at_utc0'\n                                             ,'session_ended_at_utc0'\n                                             ,'customer_seg_marlin_tag'\n                                             ,'customer_seg_gross_income_bucket'\n                                             ,'customer_seg_benefits_sensitivity_bucket'\n                                            )\n\n# Associa toda sessao que foi iniciada até 30min depois de um envio de push\ngroupedByCommunications = pushes_received.join(app_sessions, [(pushes_received.external_user_id == app_sessions.customer_id),\\\n                                                              (app_sessions.session_started_at_amsp > pushes_received.event_time_utc3),\\\n                                                              (f.col(\"session_started_at_amsp\").cast('long') - \\\n                                                               f.col(\"event_time_utc3\").cast('long')\n                                                              < 1800)]\n                                               , how = 'left')\n# Pushes diarios e pushes diários que geraram pedidos\naggregated = groupedByCommunications.withColumn('push_date', to_date('event_time_utc3', 'YYYY-MM-DD'))\\\n .withColumn('push_month', month('push_date'))\\\n .groupBy(\"external_user_id\", \"push_date\", \"push_month\")\\\n .agg(count('event_time_utc3').alias('daily_pushes'),\\\n      count('session_started_at_amsp').alias('pushes_generated_order')\n     )\n\n# get last customer_seg status in a month\nwindow = Window.partitionBy('customer_id','order_timestamp_local_month').\\\n                             orderBy(col('session_started_at_amsp').desc())\ncustomer_segment_month = app_sessions.withColumn(\"rn\",row_number().over(window).alias('rn'))\\\n                                     .where(col(\"rn\") == 1)\\\n                                     .drop(\"rn\")\\\n                                     .select('customer_id'\n                                             ,'order_timestamp_local_month'\n                                             ,'customer_seg_marlin_tag'\n                                             ,'customer_seg_gross_income_bucket'\n                                             ,'customer_seg_benefits_sensitivity_bucket'\n                                            )\n\n# Associa os customer_segments (marlin tag, income_bucket e benefits sensitivity do usuario) do mes do push\ncond = [aggregated.external_user_id == customer_segment_month.customer_id, aggregated.push_month == customer_segment_month.order_timestamp_local_month]\naggregatedWithSegmentation = aggregated.join(customer_segment_month, cond\\\n                                             , how = 'left')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":90},{"cell_type":"markdown","source":["Aqui temos uma base que mostra para cada mês e segmentos escolhidos (marlin_tag, benefits_sensitivity) qual foi a efetividade dos pushes numa janela de 30min. </br>\n**`Efectiveness_rate`** = `(# Pushes que geraram pedido até 30 min após envio do push)`/`(# Pushes enviados)`"],"metadata":{}},{"cell_type":"code","source":["comparisonSegmentationMonthly = aggregatedWithSegmentation\\\n .groupBy(\"push_month\",\"customer_seg_marlin_tag\",\"customer_seg_benefits_sensitivity_bucket\")\\\n .agg(sum('daily_pushes').alias('pushes'),\n      sum('pushes_generated_order').alias('push_attributed_orders')\n     )\\\n .withColumn(\"efectiveness_rate\", (col('push_attributed_orders')/col('pushes')) *100)\\\n .withColumnRenamed('customer_seg_marlin_tag', 'marlin_tag')\\\n .withColumnRenamed('customer_seg_benefits_sensitivity_bucket', 'benefits_sensitivity')\ndisplay(comparisonSegmentationMonthly)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>push_month</th><th>marlin_tag</th><th>benefits_sensitivity</th><th>pushes</th><th>push_attributed_orders</th><th>efectiveness_rate</th></tr></thead><tbody><tr><td>9</td><td>2. Tilapia</td><td>Baixa</td><td>31721</td><td>82</td><td>0.258503830270168</td></tr><tr><td>12</td><td>1. Marlin</td><td>Alta</td><td>149974</td><td>2937</td><td>1.9583394455038876</td></tr><tr><td>12</td><td>unknown</td><td>Alta</td><td>402687</td><td>4618</td><td>1.1467963952151423</td></tr><tr><td>9</td><td>4. Retention Carp</td><td>Media</td><td>22407</td><td>78</td><td>0.348105502744678</td></tr><tr><td>6</td><td>4. Retention Carp</td><td>Alta</td><td>251285</td><td>1175</td><td>0.46759655371391046</td></tr><tr><td>7</td><td>1. Marlin</td><td>Media</td><td>150849</td><td>1198</td><td>0.7941716550988074</td></tr><tr><td>10</td><td>3. Subsidy Carp</td><td>Media</td><td>1085</td><td>5</td><td>0.4608294930875576</td></tr><tr><td>8</td><td>4. Retention Carp</td><td>Alta</td><td>88990</td><td>385</td><td>0.4326328800988875</td></tr><tr><td>12</td><td>null</td><td>null</td><td>495045</td><td>0</td><td>0.0</td></tr><tr><td>9</td><td>1. Marlin</td><td>Media</td><td>172691</td><td>1157</td><td>0.6699828016515047</td></tr><tr><td>7</td><td>1. Marlin</td><td>Alta</td><td>95301</td><td>847</td><td>0.8887629720569563</td></tr><tr><td>6</td><td>2. Tilapia</td><td>Media</td><td>92851</td><td>517</td><td>0.5568060656320342</td></tr><tr><td>6</td><td>unknown</td><td>unknown</td><td>116079</td><td>513</td><td>0.44194040265681134</td></tr><tr><td>12</td><td>2. Tilapia</td><td>Baixa</td><td>12059</td><td>113</td><td>0.9370594576664731</td></tr><tr><td>7</td><td>4. Retention Carp</td><td>Media</td><td>81412</td><td>256</td><td>0.3144499582371149</td></tr><tr><td>7</td><td>2. Tilapia</td><td>Baixa</td><td>63176</td><td>201</td><td>0.31815879447891604</td></tr><tr><td>9</td><td>1. Marlin</td><td>Baixa</td><td>85465</td><td>483</td><td>0.5651436260457496</td></tr><tr><td>9</td><td>3. Subsidy Carp</td><td>Media</td><td>1051</td><td>4</td><td>0.3805899143672693</td></tr><tr><td>12</td><td>1. Marlin</td><td>Baixa</td><td>66277</td><td>1338</td><td>2.0187998853297526</td></tr><tr><td>7</td><td>4. Retention Carp</td><td>Baixa</td><td>42012</td><td>113</td><td>0.26897077025611726</td></tr><tr><td>9</td><td>4. Retention Carp</td><td>Baixa</td><td>11038</td><td>31</td><td>0.2808479797064686</td></tr><tr><td>8</td><td>1. Marlin</td><td>Alta</td><td>100523</td><td>806</td><td>0.8018065517344288</td></tr><tr><td>11</td><td>4. Retention Carp</td><td>Alta</td><td>48924</td><td>1682</td><td>3.437985446815469</td></tr><tr><td>10</td><td>1. Marlin</td><td>Alta</td><td>242142</td><td>1982</td><td>0.8185279711904584</td></tr><tr><td>11</td><td>4. Retention Carp</td><td>Baixa</td><td>11561</td><td>321</td><td>2.776576420724851</td></tr><tr><td>12</td><td>4. Retention Carp</td><td>Alta</td><td>20918</td><td>167</td><td>0.7983554833158046</td></tr><tr><td>8</td><td>1. Marlin</td><td>Baixa</td><td>75456</td><td>442</td><td>0.5857718405428329</td></tr><tr><td>11</td><td>3. Subsidy Carp</td><td>Alta</td><td>125321</td><td>5879</td><td>4.691153118790945</td></tr><tr><td>9</td><td>3. Subsidy Carp</td><td>Alta</td><td>144056</td><td>2013</td><td>1.3973732437385462</td></tr><tr><td>8</td><td>2. Tilapia</td><td>Media</td><td>86724</td><td>372</td><td>0.42894700428947</td></tr><tr><td>9</td><td>2. Tilapia</td><td>Alta</td><td>175568</td><td>1092</td><td>0.621981226647225</td></tr><tr><td>10</td><td>4. Retention Carp</td><td>Baixa</td><td>8435</td><td>12</td><td>0.14226437462951985</td></tr><tr><td>5</td><td>null</td><td>null</td><td>1664</td><td>0</td><td>0.0</td></tr><tr><td>9</td><td>4. Retention Carp</td><td>Alta</td><td>64484</td><td>261</td><td>0.4047515662800074</td></tr><tr><td>6</td><td>3. Subsidy Carp</td><td>Media</td><td>2120</td><td>39</td><td>1.8396226415094339</td></tr><tr><td>7</td><td>3. Subsidy Carp</td><td>Media</td><td>4502</td><td>61</td><td>1.3549533540648602</td></tr><tr><td>12</td><td>3. Subsidy Carp</td><td>Media</td><td>242</td><td>0</td><td>0.0</td></tr><tr><td>6</td><td>3. Subsidy Carp</td><td>Alta</td><td>213492</td><td>3576</td><td>1.6750042156146367</td></tr><tr><td>6</td><td>2. Tilapia</td><td>Baixa</td><td>55834</td><td>241</td><td>0.4316366371744815</td></tr><tr><td>8</td><td>4. Retention Carp</td><td>Baixa</td><td>17462</td><td>29</td><td>0.16607490550910547</td></tr><tr><td>7</td><td>2. Tilapia</td><td>Media</td><td>119824</td><td>478</td><td>0.39891841367338765</td></tr><tr><td>9</td><td>null</td><td>null</td><td>259813</td><td>0</td><td>0.0</td></tr><tr><td>10</td><td>2. Tilapia</td><td>Alta</td><td>268443</td><td>1494</td><td>0.5565427297415094</td></tr><tr><td>11</td><td>4. Retention Carp</td><td>Media</td><td>12324</td><td>308</td><td>2.4991885751379423</td></tr><tr><td>6</td><td>1. Marlin</td><td>Baixa</td><td>58973</td><td>398</td><td>0.6748851169179116</td></tr><tr><td>8</td><td>3. Subsidy Carp</td><td>Media</td><td>2178</td><td>17</td><td>0.7805325987144168</td></tr><tr><td>8</td><td>null</td><td>null</td><td>227279</td><td>0</td><td>0.0</td></tr><tr><td>6</td><td>2. Tilapia</td><td>Alta</td><td>106243</td><td>650</td><td>0.611805013036153</td></tr><tr><td>7</td><td>null</td><td>null</td><td>485294</td><td>0</td><td>0.0</td></tr><tr><td>12</td><td>3. Subsidy Carp</td><td>Alta</td><td>60187</td><td>1500</td><td>2.492232541911044</td></tr><tr><td>9</td><td>1. Marlin</td><td>Alta</td><td>142533</td><td>1100</td><td>0.7717511032532817</td></tr><tr><td>10</td><td>1. Marlin</td><td>Media</td><td>254600</td><td>1921</td><td>0.7545168892380204</td></tr><tr><td>8</td><td>4. Retention Carp</td><td>Media</td><td>35123</td><td>138</td><td>0.39290493408877375</td></tr><tr><td>12</td><td>4. Retention Carp</td><td>Baixa</td><td>4394</td><td>17</td><td>0.38689121529358217</td></tr><tr><td>11</td><td>2. Tilapia</td><td>Baixa</td><td>30200</td><td>1025</td><td>3.3940397350993377</td></tr><tr><td>8</td><td>2. Tilapia</td><td>Baixa</td><td>35473</td><td>98</td><td>0.27626645617793816</td></tr><tr><td>10</td><td>null</td><td>null</td><td>387429</td><td>1</td><td>2.5811180887336775E-4</td></tr><tr><td>8</td><td>2. Tilapia</td><td>Alta</td><td>139821</td><td>741</td><td>0.5299633102323685</td></tr><tr><td>7</td><td>4. Retention Carp</td><td>Alta</td><td>184251</td><td>862</td><td>0.46784006599692807</td></tr><tr><td>11</td><td>1. Marlin</td><td>Media</td><td>207818</td><td>13281</td><td>6.390688005851273</td></tr><tr><td>7</td><td>3. Subsidy Carp</td><td>Alta</td><td>264434</td><td>3727</td><td>1.4094254142810683</td></tr><tr><td>6</td><td>1. Marlin</td><td>Media</td><td>103398</td><td>848</td><td>0.8201319174452116</td></tr><tr><td>11</td><td>3. Subsidy Carp</td><td>Media</td><td>925</td><td>22</td><td>2.3783783783783785</td></tr><tr><td>12</td><td>2. Tilapia</td><td>Media</td><td>32596</td><td>296</td><td>0.9080868818259908</td></tr><tr><td>8</td><td>1. Marlin</td><td>Media</td><td>143058</td><td>1202</td><td>0.8402186525744803</td></tr><tr><td>6</td><td>4. Retention Carp</td><td>Media</td><td>115579</td><td>517</td><td>0.44731309320897394</td></tr><tr><td>11</td><td>1. Marlin</td><td>Baixa</td><td>91928</td><td>5802</td><td>6.311461143503612</td></tr><tr><td>7</td><td>1. Marlin</td><td>Baixa</td><td>90255</td><td>517</td><td>0.5728214503351615</td></tr><tr><td>12</td><td>unknown</td><td>Media</td><td>181617</td><td>2054</td><td>1.130951397721579</td></tr><tr><td>11</td><td>2. Tilapia</td><td>Alta</td><td>237450</td><td>8514</td><td>3.5855969677826915</td></tr><tr><td>8</td><td>3. Subsidy Carp</td><td>Alta</td><td>177295</td><td>2428</td><td>1.3694689641557856</td></tr><tr><td>12</td><td>4. Retention Carp</td><td>Media</td><td>5865</td><td>38</td><td>0.6479113384484229</td></tr><tr><td>12</td><td>2. Tilapia</td><td>Alta</td><td>117140</td><td>1479</td><td>1.2625917705309886</td></tr><tr><td>10</td><td>4. Retention Carp</td><td>Media</td><td>15985</td><td>48</td><td>0.30028151391929936</td></tr><tr><td>11</td><td>null</td><td>null</td><td>396763</td><td>0</td><td>0.0</td></tr><tr><td>9</td><td>2. Tilapia</td><td>Media</td><td>79966</td><td>354</td><td>0.4426881424605457</td></tr><tr><td>12</td><td>unknown</td><td>Baixa</td><td>90501</td><td>1096</td><td>1.2110363421398658</td></tr><tr><td>7</td><td>2. Tilapia</td><td>Alta</td><td>159757</td><td>941</td><td>0.5890195734772184</td></tr><tr><td>10</td><td>3. Subsidy Carp</td><td>Alta</td><td>147062</td><td>2234</td><td>1.5190871877167453</td></tr><tr><td>10</td><td>1. Marlin</td><td>Baixa</td><td>112710</td><td>824</td><td>0.7310797622216307</td></tr><tr><td>11</td><td>2. Tilapia</td><td>Media</td><td>72954</td><td>2384</td><td>3.2678125942374647</td></tr><tr><td>11</td><td>1. Marlin</td><td>Alta</td><td>228551</td><td>13984</td><td>6.118546845124283</td></tr><tr><td>10</td><td>2. Tilapia</td><td>Baixa</td><td>37670</td><td>129</td><td>0.34244757101141493</td></tr><tr><td>10</td><td>4. Retention Carp</td><td>Alta</td><td>46335</td><td>132</td><td>0.28488183878277756</td></tr><tr><td>6</td><td>1. Marlin</td><td>Alta</td><td>63411</td><td>585</td><td>0.9225528693759758</td></tr><tr><td>10</td><td>2. Tilapia</td><td>Media</td><td>87377</td><td>316</td><td>0.3616512354509768</td></tr><tr><td>12</td><td>1. Marlin</td><td>Media</td><td>140239</td><td>2722</td><td>1.9409721974629024</td></tr><tr><td>6</td><td>4. Retention Carp</td><td>Baixa</td><td>92247</td><td>315</td><td>0.34147451949656904</td></tr></tbody></table></div>"]}}],"execution_count":92},{"cell_type":"code","source":["df = comparisonSegmentationMonthly.toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":93},{"cell_type":"code","source":["# Removing pushes of users that were inactive in the respective month\nsensitivity_bucket = ['Baixa', 'Alta', 'Media']\nmarlin_tag = ['2. Tilapia','1. Marlin', '4. Retention Carp','3. Subsidy Carp']\ndf_filtered = df[(df['benefits_sensitivity'].isin(sensitivity_bucket)) &\\\n                  (df['marlin_tag'].isin(marlin_tag))]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":94},{"cell_type":"markdown","source":["O heatmap abaixo mostra o percentual de sessões/pedidos que foram realizado após 30 min de um envio de push. </br>Podemos ver que alguns grupos tem maior sensibilidade ao envio dos pushes. Destaque para `3. Subsidy Carp` com sensibilidade a benefícios `alta`, *as expected*. </br>\nInteressante observar o efeito de sazonalidade nos meses de `Novembro` e `Dezembro` em que tiveram altas no índice de efetividade do push chegando a `6.4%` da base `1 Marlin` com sensibilidade a benefícios `Média`."],"metadata":{}},{"cell_type":"code","source":["pt = pd.pivot_table(df_filtered, values='efectiveness_rate', \n                    index=['marlin_tag','benefits_sensitivity'],\n                    columns=['push_month'])\nfig, ax = plt.subplots(figsize=(10,10))\nax = plt.axes()\nax = sns.heatmap(pt, cmap=\"YlGnBu\", annot=True, linewidths=.5, fmt='.2g', ax=ax)\nax.set_title('Percentual de Pushes que geraram pedidos numa janela de 30min',fontsize=16, pad=15)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"markdown","source":["Vale entender o que aconteceu nos meses de Novembro e Dezembro e o quanto isso afetou as receitas/lucros do Ifood."],"metadata":{}},{"cell_type":"markdown","source":["### 7.2 EDA"],"metadata":{}},{"cell_type":"markdown","source":["#### 7.2.1 Análise de evolução\n\nO objetivo dessa etapa é identificar o comportamento das variáveis ao longo do tempo e, através de como elas se relacionam, extrair insights e hipóteses que serão úteis para responder a pergunta do problema."],"metadata":{}},{"cell_type":"code","source":["df_desc = spark.table(\"df_final_200609\")\n\ndf_desc_filtro = df_desc.select(col('customer_id')\n                                ,col('segmentation_month_month').alias('segmentation_month')\n                                ,col('ifood_status')\n                                ,col('ifood_status_last_month')\n                                ,col('marlin_tag')\n                                ,col('last_nps').alias('nps')\n                                ,col('number_of_orders').alias('orders') # Compras\n                                ,col('sum_order_total').alias('payments') # Pagamento\n                                ,col('sum_promo_is_promotion').alias('promotional_orders')\n                                ,col('sum_normal_items_quantity')\n                                ,col('sum_promo_items_quantity')\n                                ,col('sum_general_net_profit').alias('profit')\n                                ,col('sum_credit').alias('payment_discount')\n                                ,col('pushes_count_distinct_campaign_name').alias('campaingns') # Campanhas\n                                ,col('pushes_count_distinct_event_time_utc3').alias('pushes') # Pushs\n                                ,col('sum_sum_event_open').alias('visits') # Acessos\n                                                  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":100},{"cell_type":"code","source":["charts_list = [['orders', 'pushes']\n              ,['orders', 'payments']\n              ,['orders', 'campaingns']\n              ,['profit', 'payment_discount']\n              ,['orders', 'visits']\n              ,['visits', 'campaingns']\n              ,['visits', 'pushes']]\n\ntimeline_plot_sum(charts_list)"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"markdown","source":["**Highlights**:\n1. O total de pedidos por mês foi caindo com o passar dos meses;\n2. O total de pushes e campanhas mensal está aumentando com o passar dos meses;\n3. O total de payments, o montante gasto, foi caindo com o passar dos meses.\n4. Verifica-se, como antecipado, que o conceito de visitas está estritamente relacionado com o conceito de orders, uma vez que só se tem os dados das visitas que se tornaram pedidos.\n\n**Insights**:\n1. Dado o highlight 1, o número de pessoas fazendo pedidos pode estar proporcionalmente diminuindo com o passar dos meses;"],"metadata":{}},{"cell_type":"code","source":["charts_list = [['orders', 'pushes']\n              ,['orders', 'payments']\n              ,['orders', 'campaingns']\n              ,['profit', 'payment_discount']\n              ,['orders', 'payment_discount']\n              ,['orders', 'visits']\n              ,['visits', 'campaingns']\n              ,['visits', 'pushes']]\n\ntimeline_plot_avg(charts_list)"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":["**Highlights**:\n1. A quantidade média de pedidos por usuário está aumentando;\n2. A quantidade média de pushes e campanhas por usuário está aumentando com o passar dos meses;\n3. O receita mensal média por usuário (payments) está aumentando.\n\n**Insights**:\n1. O montante total de pedidos está diminuindo, mas a quantidade média de pedidos está aumentando. Portanto, o cenário que se desenha é o de menos pessoas comprando, porém, quem está comprando, está comprando mais em quantidade. Mas o que está gerando isso? \n\n**Hipóteses**: \n1. usuário naturalmente está se engajando mais com o Ifood (fora de escopo, validar com pesquisa)\n2. migração de serviço (ex. troca do Uber Eats para Ifood)  (fora de escopo, validar com pesquisa)\n3. Usuário está recebendo mais promoções e incentivos. Como isso afeta a margem de lucro? E receita?"],"metadata":{}},{"cell_type":"code","source":["charts_list = [['orders', 'orders']\n              ,['payments', 'payments']\n              ,['campaingns', 'campaingns']\n              ,['profit', 'profit']\n              ,['payment_discount', 'payment_discount']\n              ,['visits', 'visits']\n              ,['pushes', 'pushes']]\n\ntimeline_plot_mixed(charts_list)"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["df_desc_filtro_aux = df_desc_filtro.filter((df_desc_filtro.orders > 0) & (df_desc_filtro.payments > 0))\ndf_ticket = df_desc_filtro_aux.groupby('segmentation_month').agg(sum('orders').alias('orders'),sum('payments').alias('payments'),count('customer_id').alias('customers'))\ndf_ticket = df_ticket.withColumn('ticket_medio',col('payments')/col('orders'))\ndisplay(df_ticket.orderBy('segmentation_month'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>segmentation_month</th><th>orders</th><th>payments</th><th>customers</th><th>ticket_medio</th></tr></thead><tbody><tr><td>6</td><td>88052</td><td>4882892.110000031</td><td>29726</td><td>55.45464168900231</td></tr><tr><td>7</td><td>69838</td><td>3829942.4900000077</td><td>18411</td><td>54.84038045190309</td></tr><tr><td>8</td><td>67281</td><td>3753502.740000007</td><td>17301</td><td>55.78845052838108</td></tr><tr><td>9</td><td>65700</td><td>3654919.8700000057</td><td>16839</td><td>55.63043942161348</td></tr><tr><td>10</td><td>66549</td><td>3689285.3200000087</td><td>16606</td><td>55.437126327969</td></tr><tr><td>11</td><td>69398</td><td>3894036.6300000036</td><td>16761</td><td>56.111654946828494</td></tr><tr><td>12</td><td>64588</td><td>3730970.560000005</td><td>16250</td><td>57.765692698334135</td></tr></tbody></table></div>"]}}],"execution_count":106},{"cell_type":"markdown","source":["#### 7.2.2 Validação de idéias e insights"],"metadata":{}},{"cell_type":"code","source":["df_desc_filtro = df_desc_filtro.fillna(0, subset=['payments'])\ndf_desc_filtro = df_desc_filtro.fillna(0, subset=['promotional_orders'])\ndf_desc_filtro = df_desc_filtro.fillna(0, subset=['orders'])\n\n\ndf_desc_filtro = df_desc_filtro.withColumn('payments_last_month',\n                  f.lag(df_desc_filtro['payments'])\n                   .over(Window.partitionBy(\"customer_id\")\n                   .orderBy(\"segmentation_month\")))\n\ndf_desc_filtro = df_desc_filtro.withColumn('marlin_tag_last_month',\n                  f.lag(df_desc_filtro['marlin_tag'])\n                   .over(Window.partitionBy(\"customer_id\")\n                   .orderBy(\"segmentation_month\")))\n\ndf_desc_filtro = df_desc_filtro.withColumn('ifood_status_group',\\\n                                           when(df_desc_filtro.ifood_status == 'Inactive','Inactive').when(df_desc_filtro.ifood_status == 'Churn','Inactive').otherwise('Active'))\n\ndf_desc_filtro = df_desc_filtro.withColumn('marlin_tag_group',\\\n                                           when(df_desc_filtro.marlin_tag == '1. Marlin','Marlin').otherwise('Other'))\n\ndf_desc_filtro = df_desc_filtro.withColumn('ifood_status_group_next_month',\n                  f.lag(df_desc_filtro['ifood_status_group'])\n                   .over(Window.partitionBy(\"customer_id\")\n                   .orderBy(desc(\"segmentation_month\"))))\n\ndf_desc_filtro = df_desc_filtro.withColumn('nps_next_month',\n                  f.lag(df_desc_filtro['nps'])\n                   .over(Window.partitionBy(\"customer_id\")\n                   .orderBy(desc(\"segmentation_month\"))))\n\ndf_desc_filtro = df_desc_filtro.withColumn('normal_orders',\\\n                                           col('orders') - col('promotional_orders'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":108},{"cell_type":"markdown","source":["##### Ifood Status\n\nFoi levantada a hipótese de que a quantidade de pessoas comprando está diminuindo e o primeiro gráfico abaixo comprova isso, dado que **vê-se a proporção de pessoas Inativas aumentar com o passar dos meses**.\n\nJá o segundo e terceiro gráficos abaixo mostram que a quantidade de pushes e campanhas recebidas pelos ativos é maior para ativos do que para inativos. Isso parece fazer sentido quando o objetivo é fazer com que as pessoas se mantenham ativas. Por outro lado, mesmo não entrando no mérito da quantidade ideal de pushes/campanhas, faz sentido o Ifood querer que os inativos voltem a ser ativos. De qualquer maneira, nesse caso, isso parece estar relacionado com decisões de negócio do Ifood e não diretamente com a vida do cliente.\n\nPor fim, mas definitivamente não menos importante, omparando quem é ativos no mês atual e que se tornarão inativos no mês seguinte com quem ativo no atual e se mantém ativo no mês seguinte, vemos que o primeiro grupo (curva em laranja) recebe, em média, menos pushes que o segundo grupo em todos os meses. Isso é, os **clientes ativos que permanecem ativos rebecem mais pushes que os que decidem dar Churn**."],"metadata":{}},{"cell_type":"code","source":["display(df_desc_filtro.groupby('ifood_status_group','segmentation_month').count().orderBy('segmentation_month'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ifood_status_group</th><th>segmentation_month</th><th>count</th></tr></thead><tbody><tr><td>Active</td><td>6</td><td>29282</td></tr><tr><td>Inactive</td><td>6</td><td>444</td></tr><tr><td>Active</td><td>7</td><td>18253</td></tr><tr><td>Inactive</td><td>7</td><td>11710</td></tr><tr><td>Active</td><td>8</td><td>17018</td></tr><tr><td>Inactive</td><td>8</td><td>12945</td></tr><tr><td>Inactive</td><td>9</td><td>13245</td></tr><tr><td>Active</td><td>9</td><td>16718</td></tr><tr><td>Inactive</td><td>10</td><td>13357</td></tr><tr><td>Active</td><td>10</td><td>16606</td></tr><tr><td>Active</td><td>11</td><td>16524</td></tr><tr><td>Inactive</td><td>11</td><td>13439</td></tr><tr><td>Active</td><td>12</td><td>16185</td></tr><tr><td>Inactive</td><td>12</td><td>13778</td></tr></tbody></table></div>"]}}],"execution_count":110},{"cell_type":"code","source":["display(df_desc_filtro.groupby('ifood_status_group','segmentation_month').agg(avg('pushes')).orderBy('segmentation_month'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ifood_status_group</th><th>segmentation_month</th><th>avg(pushes)</th></tr></thead><tbody><tr><td>Inactive</td><td>6</td><td>34.86479591836735</td></tr><tr><td>Active</td><td>6</td><td>46.434839141095175</td></tr><tr><td>Active</td><td>7</td><td>73.35063703703703</td></tr><tr><td>Inactive</td><td>7</td><td>54.12698587819947</td></tr><tr><td>Active</td><td>8</td><td>56.26104981185024</td></tr><tr><td>Inactive</td><td>8</td><td>27.12707182320442</td></tr><tr><td>Inactive</td><td>9</td><td>29.773767505408173</td></tr><tr><td>Active</td><td>9</td><td>58.62583102046085</td></tr><tr><td>Active</td><td>10</td><td>78.48642136112015</td></tr><tr><td>Inactive</td><td>10</td><td>43.31853020739405</td></tr><tr><td>Active</td><td>11</td><td>68.16888480954566</td></tr><tr><td>Inactive</td><td>11</td><td>47.4343658679135</td></tr><tr><td>Inactive</td><td>12</td><td>55.870598788966134</td></tr><tr><td>Active</td><td>12</td><td>85.01109403194546</td></tr></tbody></table></div>"]}}],"execution_count":111},{"cell_type":"code","source":["display(df_desc_filtro.groupby('ifood_status_group','segmentation_month').agg(avg('campaingns')).orderBy('segmentation_month'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ifood_status_group</th><th>segmentation_month</th><th>avg(campaingns)</th></tr></thead><tbody><tr><td>Inactive</td><td>6</td><td>31.742346938775512</td></tr><tr><td>Active</td><td>6</td><td>36.33096812541793</td></tr><tr><td>Inactive</td><td>7</td><td>48.50794351279788</td></tr><tr><td>Active</td><td>7</td><td>55.05232592592593</td></tr><tr><td>Active</td><td>8</td><td>37.55284138019006</td></tr><tr><td>Inactive</td><td>8</td><td>23.135244014732965</td></tr><tr><td>Inactive</td><td>9</td><td>25.77536149379483</td></tr><tr><td>Active</td><td>9</td><td>43.07603433808817</td></tr><tr><td>Inactive</td><td>10</td><td>39.61451758340848</td></tr><tr><td>Active</td><td>10</td><td>64.57225659394334</td></tr><tr><td>Active</td><td>11</td><td>55.72700452370026</td></tr><tr><td>Inactive</td><td>11</td><td>45.96329631794272</td></tr><tr><td>Inactive</td><td>12</td><td>53.70923973985199</td></tr><tr><td>Active</td><td>12</td><td>72.06054935507585</td></tr></tbody></table></div>"]}}],"execution_count":112},{"cell_type":"code","source":["data_1 = df_desc_filtro.filter((df_desc_filtro.ifood_status_group != 'Inactive') & (df_desc_filtro.ifood_status_group_next_month == 'Inactive')).groupby('segmentation_month').agg(avg('pushes').alias('pushes_active_inactive')).orderBy('segmentation_month')\ndata_2 = df_desc_filtro.filter((df_desc_filtro.ifood_status_group != 'Inactive') & (df_desc_filtro.ifood_status_group_next_month != 'Inactive')).groupby('segmentation_month').agg(avg('pushes').alias('pushes_active_active')).orderBy('segmentation_month')\n\ndf_aux_status = join_removing_repeated(data_1,data_2,data_1.segmentation_month == data_2.segmentation_month,'left')\n\ndisplay(df_aux_status)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>pushes_active_inactive</th><th>segmentation_month</th><th>pushes_active_active</th></tr></thead><tbody><tr><td>36.53080111828786</td><td>6</td><td>52.64424297370807</td></tr><tr><td>63.16398865784499</td><td>7</td><td>76.7604207862058</td></tr><tr><td>45.6720741599073</td><td>8</td><td>59.250592950028626</td></tr><tr><td>46.32355665328805</td><td>9</td><td>61.877590990696916</td></tr><tr><td>62.18986938515451</td><td>10</td><td>82.67395219384414</td></tr><tr><td>58.71873985060085</td><td>11</td><td>70.55897815015607</td></tr></tbody></table></div>"]}}],"execution_count":113},{"cell_type":"markdown","source":["**Promotions**\n\nFoi levantada a hipótese de que o aumento na quantidade média e total de campanhas/pushes poderia estar contribuindo para a diminuição total e média de payments. Vê-se que **a proporção de pedidos promocionais (promotional_orders) aumentou 7p.p (25%) com o passar do tempo**."],"metadata":{}},{"cell_type":"code","source":["timeline_plot_avg([['promotional_orders','normal_orders']])"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["display(df_desc_filtro.groupby('segmentation_month').agg(sum('promotional_orders').alias('promotional_orders'),sum('normal_orders').alias('normal_orders')).orderBy('segmentation_month'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>segmentation_month</th><th>promotional_orders</th><th>normal_orders</th></tr></thead><tbody><tr><td>6</td><td>24231.0</td><td>63821.0</td></tr><tr><td>7</td><td>20510.0</td><td>49328.0</td></tr><tr><td>8</td><td>21882.0</td><td>45399.0</td></tr><tr><td>9</td><td>22539.0</td><td>43161.0</td></tr><tr><td>10</td><td>23022.0</td><td>43527.0</td></tr><tr><td>11</td><td>24613.0</td><td>44785.0</td></tr><tr><td>12</td><td>21778.0</td><td>42810.0</td></tr></tbody></table></div>"]}}],"execution_count":116},{"cell_type":"markdown","source":["**Marlin Tag**\n\nDado o conceito de que os Marlins são os melhores clientes, decidiu-se testar se eles fazem mais pedidos e gastam mais que os demais para justificar a tag recebida. E sim, os dois primeiros gráficos abaixo evidenciam justamente isso. Já o terceito gráfico mostra que, **além de gastar e pedir mais, os Marlin também recebem mais pushes/campanhas**.\n\nNão há evidências para afirmar se são esses pushes que os incentivam a comprar/gastar mais, ou se eles gastarem/comprarem mais é que influencia o Ifood a enviar mais pushes e campanhas para eles."],"metadata":{}},{"cell_type":"code","source":["display(df_desc_filtro.groupby('marlin_tag_group','segmentation_month').agg(avg('orders')).orderBy('segmentation_month'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>marlin_tag_group</th><th>segmentation_month</th><th>avg(orders)</th></tr></thead><tbody><tr><td>Marlin</td><td>6</td><td>6.294819949932601</td></tr><tr><td>Other</td><td>6</td><td>2.2566746830799334</td></tr><tr><td>Other</td><td>7</td><td>1.4858355119358848</td></tr><tr><td>Marlin</td><td>7</td><td>5.304524886877828</td></tr><tr><td>Marlin</td><td>8</td><td>5.050147856086742</td></tr><tr><td>Other</td><td>8</td><td>1.203551975099556</td></tr><tr><td>Marlin</td><td>9</td><td>4.583664090244412</td></tr><tr><td>Other</td><td>9</td><td>1.0699887194075237</td></tr><tr><td>Other</td><td>10</td><td>1.0411755930180515</td></tr><tr><td>Marlin</td><td>10</td><td>4.628780190785468</td></tr><tr><td>Marlin</td><td>11</td><td>4.8291993720565145</td></tr><tr><td>Other</td><td>11</td><td>1.1395041160329282</td></tr><tr><td>Other</td><td>12</td><td>1.0979719942056978</td></tr><tr><td>Marlin</td><td>12</td><td>4.52274937857992</td></tr></tbody></table></div>"]}}],"execution_count":118},{"cell_type":"code","source":["display(df_desc_filtro.groupby('marlin_tag_group','segmentation_month').agg(avg('payments')).orderBy('segmentation_month'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>marlin_tag_group</th><th>segmentation_month</th><th>avg(payments)</th></tr></thead><tbody><tr><td>Other</td><td>6</td><td>118.13597399421198</td></tr><tr><td>Marlin</td><td>6</td><td>382.1802927017137</td></tr><tr><td>Other</td><td>7</td><td>74.33763596622781</td></tr><tr><td>Marlin</td><td>7</td><td>316.0516485671189</td></tr><tr><td>Other</td><td>8</td><td>59.74296837094322</td></tr><tr><td>Marlin</td><td>8</td><td>301.6631481025136</td></tr><tr><td>Marlin</td><td>9</td><td>271.5816868602465</td></tr><tr><td>Other</td><td>9</td><td>51.733621070184824</td></tr><tr><td>Marlin</td><td>10</td><td>272.91229247006305</td></tr><tr><td>Other</td><td>10</td><td>49.72935451787747</td></tr><tr><td>Marlin</td><td>11</td><td>291.00310727367867</td></tr><tr><td>Other</td><td>11</td><td>54.56203155625236</td></tr><tr><td>Other</td><td>12</td><td>54.803129888942465</td></tr><tr><td>Marlin</td><td>12</td><td>280.55741273100625</td></tr></tbody></table></div>"]}}],"execution_count":119},{"cell_type":"code","source":["display(df_desc_filtro.groupby('marlin_tag_group','segmentation_month').agg(avg('pushes')).orderBy('segmentation_month'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>marlin_tag_group</th><th>segmentation_month</th><th>avg(pushes)</th></tr></thead><tbody><tr><td>Other</td><td>6</td><td>43.72164402892926</td></tr><tr><td>Marlin</td><td>6</td><td>58.73284419507984</td></tr><tr><td>Marlin</td><td>7</td><td>75.73600271462504</td></tr><tr><td>Other</td><td>7</td><td>63.95664754302819</td></tr><tr><td>Marlin</td><td>8</td><td>58.40099972229936</td></tr><tr><td>Other</td><td>8</td><td>40.617127876492866</td></tr><tr><td>Marlin</td><td>9</td><td>60.25962104272096</td></tr><tr><td>Other</td><td>9</td><td>41.686228531592626</td></tr><tr><td>Other</td><td>10</td><td>56.93451304572462</td></tr><tr><td>Marlin</td><td>10</td><td>80.96843188836783</td></tr><tr><td>Marlin</td><td>11</td><td>67.87836879432624</td></tr><tr><td>Other</td><td>11</td><td>56.771566327860306</td></tr><tr><td>Other</td><td>12</td><td>68.11778570518537</td></tr><tr><td>Marlin</td><td>12</td><td>85.6607601124282</td></tr></tbody></table></div>"]}}],"execution_count":120},{"cell_type":"markdown","source":["**NPS**\n\nComparando quem é classificado como promotor no mês atual e detrator no mês seguinte com quem é promotor no atual e se mantém promotor no mês seguinte, vemos que o primeiro grupo (curva em azul) recebe, em média, mais pushes que o segundo grupo em praticamente todos os meses. Isso é, **receber pushes não parece ter feito com que os clientes ficassem mais felizes, pelo contrário**."],"metadata":{}},{"cell_type":"code","source":["data_1 = df_desc_filtro.filter((df_desc_filtro.nps != 'Sem Avaliacoes') & (df_desc_filtro.nps != 'Detractor')& (df_desc_filtro.nps_next_month == 'Detractor')).groupby('segmentation_month').agg(avg('pushes').alias('pushes_promoter_detractor')).orderBy('segmentation_month')\ndata_2 = df_desc_filtro.filter((df_desc_filtro.nps != 'Sem Avaliacoes') & (df_desc_filtro.nps != 'Detractor') & (df_desc_filtro.nps_next_month != 'Detractor') & (df_desc_filtro.nps_next_month != 'Sem Avaliacoes')).groupby('segmentation_month').agg(avg('pushes').alias('pushes_promoter_promoter')).orderBy('segmentation_month')\n\ndf_aux_nps = join_removing_repeated(data_1,data_2,data_1.segmentation_month == data_2.segmentation_month,'left')\n\ndisplay(df_aux_nps)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>pushes_promoter_detractor</th><th>segmentation_month</th><th>pushes_promoter_promoter</th></tr></thead><tbody><tr><td>53.13461538461539</td><td>6</td><td>49.19781410893325</td></tr><tr><td>73.34482758620689</td><td>7</td><td>68.25280952838344</td></tr><tr><td>52.57303370786517</td><td>8</td><td>49.63928029481899</td></tr><tr><td>58.025</td><td>9</td><td>51.573493126542125</td></tr><tr><td>69.75342465753425</td><td>10</td><td>70.35210571816722</td></tr><tr><td>68.39344262295081</td><td>11</td><td>63.498139419498884</td></tr></tbody></table></div>"]}}],"execution_count":122},{"cell_type":"markdown","source":["### 7.3 Respostas e conclusões"],"metadata":{}},{"cell_type":"markdown","source":["**Introdução**: Como destacado inicialmente, o conceito de importância não está definido na pergunta, então buscamos formas de identificar como os eventos se relacionam e quais as principais argumentações que poderiam ser propostas para o conceito de importância que tomamos. Vale evidenciar também que não foram realizados testes estatísticos/matemáticos para testar/validar as hipóteses levantadas dado que isso, de fato, não é o que nos propusemos a trazer com essa análise, além das limitações relacionadas aos dados disponíveis.\n\n**Principais Insights**:\n- O total de pedidos por mês foi caindo com o passar dos meses porém a quantidade média de medidos por usuário aumentou. Ou seja, com o passar do tempo os que compram acabam fazendo mais pedidos.\n- Podemos observar um comportamento semelhante em receita: apesar do total de payments e o montante gasto cair com o passar dos meses, a receita mensal média por usuário aumentou. Porém, o ticket médio se manteve constante. Ou seja, o aumento de receita ocorre pelo aumento de pedidos médios do usuário no mês.\n- Na perspectiva de pushes, o total de pushes e campanhas mensal (total e médio) está aumentando com o passar dos meses. Além disso, o usuário também está recebendo mais promoções e incentivos.\n- Olhando pela taxa de efetividade do push (percentual de pushes que geraram um pedido em uma janela de até 30min), vemos que em junho a taxa é maior e com o passar dos meses ela cai. Há uma recuperação de outubro para frente.\n- A partir de outubro vemos um aumento de mais +33% no total e na média de pushes mensais. Além disso, em novembro e dezembro há um aumento no total e na média de desconto no mês (de ~R$420k para ~R$440k, ou ~R$25 para quase R$27 de desconto/mês).\n- Em termos de lucro, isso se refletiu positivamente em novembro, com uma média de lucro por cliente de ~+R$32. Já em dezembro, temos a máxima histórica de pushes (+1.8M, média de 75 pushes/mês). A curva de lucro acompanhou novembro pois o mês teve maior quantidade de inativos (46%), uma média de pedidos/usuário menor (de 4.2 para 4) e um desconto médio maior (~R$27).\n\n\n\n**Apêndice**:\n- É possível também ver para todos os meses que usuários que se tornaram inativos no mês seguinte, receberam em média menos pushes no mês atual.\n- Isso implica que devemos enviar mais pushes? Não necessariamente, pois usuários que se tornaram detratores receberam em média mais pushes no mês atual. Há um trade-off em relação a percepção de marca e serviço (clientes se tornam detratores) e o churn."],"metadata":{}},{"cell_type":"markdown","source":["## 8. [PROBLEMA 3] Prever a valor total de pedidos por cliente no mês seguinte."],"metadata":{}},{"cell_type":"markdown","source":["### 8.1 Pipeline"],"metadata":{}},{"cell_type":"code","source":["df_final = spark.table(\"df_final_200609\")\n\n# Cria a Target com 1 para cliente que foi Churn. 0 caso contrário\ndf_final = df_final.withColumn('target_current',when(df_final.ifood_status == 'Churn',1).otherwise(0))\n\ndf_final = df_final.withColumn('target',\n                  f.lag(df_final['target_current'])\n                   .over(Window.partitionBy(\"customer_id\")\n                   .orderBy(desc(\"segmentation_month_month\"))))\n\n# Para o lagging do problema 3, o churn/inativo do mês seguinte significa que o cliente nao fez nenhum pedido. Por exemplo, quem ficou inativo/churn tem sum_order_total null. Nesse caso, queremos prever que esse usuário vai ter um sum_order_total de zero.\ndf_final = df_final.fillna(0, subset=['sum_order_total'])\ndf_final = df_final.withColumn('target_3',\n                  f.lag(df_final['sum_order_total'])\n                   .over(Window.partitionBy(\"customer_id\")\n                   .orderBy(desc(\"segmentation_month_month\"))))\n\n# df_final = df_final.fillna(0, subset=['number_of_orders'])\n# df_final = df_final.withColumn('target_3',\n#                   f.lag(df_final['number_of_orders'])\n#                    .over(Window.partitionBy(\"customer_id\")\n#                    .orderBy(desc(\"segmentation_month_month\"))))\n\ndf_final = df_final.filter(~df_final.ifood_status.isin(['Churn', 'Inactive'])) # como o objetivo é prever churn, mantemos somente quem pode dar churn\n\n# PREPARATION:\n# Listas de variaveis por tipo: categorico, numerico e data\ncategorical_columns = df_final.select(*[x[0] for x in df_final.dtypes if x[1] not in ('double', 'long', 'int', 'bigint', 'date')]).columns\nnumerical_columns = df_final.select(*[x[0] for x in df_final.dtypes if x[1] in ('double', 'long', 'int', 'bigint')]).columns\ndate_columns = df_final.select(*[x[0] for x in df_final.dtypes if x[1] in ('date')]).columns\n\n# Exclusão de colunas com dados referentes ao mês.\nexcluded_columns = [\n  'last_invalid_order_date'\n  ,'preferred_shift_bucket_description' # redundante pois preferred_shift_bucket foi tratado\n  ,'external_user_id' # chave da tabela de pushes\n  ,'event_month' # chave da tabela de pushes\n  ,'days_to_reorder_at_concluded' # variavel da tabela de segmentation que optamos por nao usar\n  ,'days_to_reorder_at_datasource' # variavel da tabela de segmentation que optamos por nao usar\n  ,'registration_month'\n  ,'registration_dayofweek'\n  ,'first_order_month'\n  ,'first_order_dayofweek'\n  ,'segmentation_month_dayofweek'\n  ,'segmentation_month' # redundante\n  ,'last_order_month'\n  ,'last_order_dayofweek'\n  ,'order_timestamp_local_month' # chave do join da base de order\n  ,'count_distinct_event_dayofweek' # nao tem uma interpretacao. Um 7 diz somente se um usuario recebeu num mes pushes todos os dias da semana\n  ,'event_month' # chave do join da base de pushes\n  ,'pushes_count_distinct_event_time_utc3' # timestamp da base de pushes\n  ,'sum_distance_merchant_customer' # from Orders+Visits: Nulo não faz sentido\n  ,'avg_distance_merchant_customer'\n  ,'most_common_platform'\n  ,'most_common_merchant_dish_type'\n  ,'most_common_customer_state_label'\n]\n\nincluded_categorical = [\n 'marlin_tag',\n 'last_nps',\n 'benefits_sensitivity_bucket',\n 'merchant_variety_bucket',\n 'most_common_order_shift',\n 'most_common_delivery_type',\n 'most_common_device_platform',\n 'most_common_payment_method',\n 'most_common_customer_seg_recency_bucket',\n 'most_common_customer_seg_merchant_offer_bucket',\n 'most_common_customer_seg_benefits_sensitivity_bucket',\n 'most_common_customer_seg_frequency_bucket',\n 'most_common_customer_seg_gross_income_bucket'\n]\n\nexcluded_columns = list(set(excluded_columns + date_columns + categorical_columns))\nincluded_columns = list(set(df_final.columns) - set(excluded_columns)) + ['customer_id'] + included_categorical\n\ndf_filtrado = df_final[included_columns]\n\n# Inputar zero para colunas numericas de pushes (clientes ativos que nao receberam push no mes) [~10050]\npushes_fillna_list = [\n  'pushes_changed_platform'\n  ,'pushes_count_distinct_event_date'\n  ,'pushes_count_distinct_campaign_name'\n]\nfor coluna in pushes_fillna_list:\n  df_filtrado = df_filtrado.fillna(0, subset=[coluna])\n\n# Drop em variaveis relacionadas a pedidos/sessões de clientes que nao fizeram compras (~3903)\ndf_filtrado = df_filtrado.dropna()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":127},{"cell_type":"code","source":["# df_filtrado_200610: target_3 is sum_order_total next month\n# df_filtrado_200610b: target_3 is number_of_orders next month\n# df_filtrado_200614: version with state and favorite dishes.\ndf_filtrado.write.saveAsTable('df_filtrado_200614')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":128},{"cell_type":"code","source":["included_categorical = [\n 'marlin_tag',\n 'last_nps',\n 'benefits_sensitivity_bucket',\n 'merchant_variety_bucket',\n 'most_common_order_shift',\n 'most_common_delivery_type',\n 'most_common_device_platform',\n 'most_common_payment_method',\n 'most_common_customer_seg_recency_bucket',\n 'most_common_customer_seg_merchant_offer_bucket',\n 'most_common_customer_seg_benefits_sensitivity_bucket',\n 'most_common_customer_seg_frequency_bucket',\n 'most_common_customer_seg_gross_income_bucket'\n]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":129},{"cell_type":"code","source":["df_filtrado = spark.table(\"df_filtrado_200610\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":130},{"cell_type":"code","source":["stop_aux = 4\ncount_aux = 1\n#excluido_count = 999999\n\n#while (count_aux <= stop_aux) | (excluido_count == 0):\nwhile count_aux <= stop_aux:\n  \n  count_aux = count_aux + 1\n  \n  resumo = df_filtrado.select('target_3').summary().collect()\n  q1 = float(resumo[4][1])\n  q3 = float(resumo[6][1])\n\n  lim_inf = q1 - 1.5 * (q3 - q1)\n  lim_sup = q3 + 1.5 * (q3 - q1)\n\n  df_filtrado = df_filtrado.filter(\n    (df_filtrado['target_3'] < lim_sup) &\n    (df_filtrado['target_3'] > lim_inf)\n  )\n\n  resumo = df_filtrado.select('sum_order_total').summary().collect()\n  q1 = float(resumo[4][1])\n  q3 = float(resumo[6][1])\n\n  lim_inf = q1 - 1.5 * (q3 - q1)\n  lim_sup = q3 + 1.5 * (q3 - q1)\n\n  df_filtrado = df_filtrado.filter(\n    (df_filtrado['sum_order_total'] < lim_sup) &\n    (df_filtrado['sum_order_total'] > lim_inf)\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":131},{"cell_type":"code","source":["df_filtrado.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[84]: 89257</div>"]}}],"execution_count":132},{"cell_type":"code","source":["# PIPELINE BUILDING:\n# Based on https://gist.github.com/colbyford/83978917799dbcab6293521a60f29e94\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, OneHotEncoderEstimator, StringIndexer, VectorAssembler, MinMaxScaler, Normalizer\n\n#df_filtrado = spark.table(\"df_filtrado_200610\")\n\ncategoricalColumns = included_categorical\n\nnumericalColumns = list(set(df_filtrado.columns) - set(['customer_id','segmentation_month_month','ifood_status','ifood_status_last_month','target','target_3']) - set(categoricalColumns))\n\ncategoricalColumnsclassVec = [c + \"classVec\" for c in categoricalColumns]\n\nstages = []\n\nfor categoricalColumn in categoricalColumns:\n  print(categoricalColumn)\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalColumn, outputCol = categoricalColumn+\"Index\").setHandleInvalid(\"skip\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalColumn+\"Index\", outputCol=categoricalColumn+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]\n\n# Convert label into label indices using the StringIndexer\n#label_stringIndexer = StringIndexer(inputCol = label, outputCol = \"label\").setHandleInvalid(\"skip\")\n#stages += [label_stringIndexer]\n\n# Transform all features into a vector using VectorAssembler\nassemblerInputs = categoricalColumnsclassVec + numericalColumns \n# assembler only considers 'classVec' columns (it already did not consider stringIndexer)\nassembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")\n# assembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"featuresAssembled\")\nstages += [assembler]\n\nprepPipeline = Pipeline().setStages(stages)\npipelineModel = prepPipeline.fit(df_filtrado)\ndataset = pipelineModel.transform(df_filtrado)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">marlin_tag\nlast_nps\nbenefits_sensitivity_bucket\nmerchant_variety_bucket\nmost_common_order_shift\nmost_common_delivery_type\nmost_common_device_platform\nmost_common_payment_method\nmost_common_customer_seg_recency_bucket\nmost_common_customer_seg_merchant_offer_bucket\nmost_common_customer_seg_benefits_sensitivity_bucket\nmost_common_customer_seg_frequency_bucket\nmost_common_customer_seg_gross_income_bucket\n</div>"]}}],"execution_count":133},{"cell_type":"code","source":["# dataset_churn_200610: target_3 is sum_order_total next month\n# dataset_churn_200610b: target_3 is number_of_orders next month\n# dataset_churn_200614: version with states and favorite dishes\n# dataset_churn_200617_sem_outliers: version without states and favorite dishes, removing outliers\ndataset.write.saveAsTable('dataset_churn_200617_sem_outliers')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":134},{"cell_type":"markdown","source":["### 8.2 Modelagem"],"metadata":{}},{"cell_type":"code","source":["target = 'target_3'\ndataset = spark.table(\"dataset_churn_200617_sem_outliers\").repartition(2).cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":136},{"cell_type":"markdown","source":["#### 8.2.1 Regressão Logística"],"metadata":{}},{"cell_type":"code","source":["# TREINAMENTO E TESTE DO MODELO:\nfrom pyspark.ml.regression import LinearRegression\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed=42)\n\nlr = LinearRegression(labelCol=target)\n\nmodelo_lr = lr.fit(treino)\n\nprint('==============================')\nprint('Métricas no conjunto de TREINO')\nprint('==============================')\n#print(f'Coeficientes: {modelo_lr.coefficients}')\n#print(f'pValues: {modelo.summary.pValues}')\nprint(f'Intercepto: {modelo_lr.intercept}')\nprint(f'MAE: {modelo_lr.summary.meanAbsoluteError}')\nprint(f'RMSE: {modelo_lr.summary.rootMeanSquaredError}')\nprint(f'r2 Ajustado: {modelo_lr.summary.r2adj}')\n\npredictions_lr = modelo_lr.transform(treino)\n\nprint('=============================')\nprint('Métricas no conjunto de TESTE')\nprint('=============================')\nresultado_teste = modelo_lr.evaluate(teste)\n\nprint(f'MAE: {resultado_teste.meanAbsoluteError}')\nprint(f'RMSE: {resultado_teste.rootMeanSquaredError}')\nprint(f'r2 Ajustado: {resultado_teste.r2adj}')\n\npredictions_lr = modelo_lr.transform(teste)\n#mape = compute_mape(predicoes, 'number_of_orders')\n#print(f'MAPE: {mape}')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">==============================\nMétricas no conjunto de TREINO\n==============================\nIntercepto: 14.875017405079742\nMAE: 68.90758772292205\nRMSE: 88.47738542952085\nr2 Ajustado: 0.25506714739875913\n=============================\nMétricas no conjunto de TESTE\n=============================\nMAE: 69.16363463013249\nRMSE: 89.15706590190861\nr2 Ajustado: 0.24751378628290854\n</div>"]}}],"execution_count":138},{"cell_type":"code","source":["mape = compute_mape(predictions_lr, y_true=target, y_pred='prediction')\nprint(f'MAPE: {mape}')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MAPE: 59.3831\n</div>"]}}],"execution_count":139},{"cell_type":"markdown","source":["Com outliers </br>\nMAE: 107.17590288465296</br>\nRMSE: 168.8219963148754</br>\nr2 Ajustado: 0.6077634317813403</br>"],"metadata":{}},{"cell_type":"code","source":["reg_model_evaluator(predictions_lr, labelCol=target)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MAE: 106.90011314802732\nRMSE: 167.25215380656473\nr2 Ajustado: 0.5769579847112065\n</div>"]}}],"execution_count":141},{"cell_type":"markdown","source":["#### 8.2.2 Decision Tree Regressor"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.mllib.util import MLUtils\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed=42)\n\ndt = DecisionTreeRegressor(labelCol=target)\n\nmodelo_dt = dt.fit(treino)\n\nprint('==============================')\nprint('Métricas no conjunto de TREINO')\nprint('==============================')\n\npredictions_dt = modelo_dt.transform(treino)\n\nreg_model_evaluator(predictions_dt, labelCol=target)\n\nprint(modelo_dt)\n\nprint('=============================')\nprint('Métricas no conjunto de TESTE')\nprint('=============================')\n\npredictions_dt = modelo_dt.transform(teste)\n\n# Exemplos de predições feitas:\npredictions_dt.select(\"prediction\", target, \"features\").show(5)\n\nreg_model_evaluator(predictions_dt, labelCol=target)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">==============================\nMétricas no conjunto de TREINO\n==============================\nMAE: 70.22382900473575\nRMSE: 89.71656606151157\nr2 Ajustado: 0.2351347580137737\nDecisionTreeRegressionModel (uid=DecisionTreeRegressor_077a3c2f6e19) of depth 5 with 63 nodes\n=============================\nMétricas no conjunto de TESTE\n=============================\n+------------------+--------+--------------------+\n        prediction|target_3|            features|\n+------------------+--------+--------------------+\n 35.33443652728299|     0.0|(101,[0,4,7,10,11...|\n 72.73391101914125|     0.0|(101,[1,4,6,12,20...|\n121.09698124999996|     0.0|(101,[1,6,13,20,2...|\n48.123793378540064|     0.0|(101,[0,4,7,8,13,...|\n48.123793378540064|     0.0|(101,[2,4,6,9,13,...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nMAE: 70.70507601969265\nRMSE: 90.64620537044239\nr2 Ajustado: 0.22662074884649974\n</div>"]}}],"execution_count":143},{"cell_type":"markdown","source":["Sem outliers:</br>\nMAE: 112.54892549073273</br>\nRMSE: 181.87758983896944</br>\nr2 Ajustado: 0.5468231792861152"],"metadata":{}},{"cell_type":"code","source":["ExtractFeatureImp(modelo_dt.featureImportances, predictions_dt, \"features\").head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>name</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>53</td>\n      <td>orders_last_91d</td>\n      <td>0.541936</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>93</td>\n      <td>sum_paid_amount</td>\n      <td>0.176984</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>56</td>\n      <td>qtt_orders_last_year</td>\n      <td>0.161193</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>94</td>\n      <td>sum_order_total</td>\n      <td>0.052837</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>58</td>\n      <td>qtt_valid_orders</td>\n      <td>0.047376</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>72</td>\n      <td>avg_aov_last_91d</td>\n      <td>0.009982</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>49</td>\n      <td>recency_days</td>\n      <td>0.009691</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>16</td>\n      <td>most_common_order_shiftclassVec_weekend dawn</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>13</td>\n      <td>most_common_order_shiftclassVec_weekday lunch</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>14</td>\n      <td>most_common_order_shiftclassVec_weekend lunch</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":145},{"cell_type":"markdown","source":["#### 8.2.3 Random Forest Regressor"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed=42)\n\nrf = RandomForestRegressor(labelCol=target)\n\nmodelo_rf = rf.fit(treino)\n\nprint('==============================')\n\nprint('Métricas no conjunto de TREINO')\nprint('==============================')\n\npredictions_rf = modelo_rf.transform(treino)\n\nreg_model_evaluator(predictions_rf, labelCol=target)\n\nprint(modelo_rf)\n\nprint('=============================')\nprint('Métricas no conjunto de TESTE')\nprint('=============================')\n\npredictions_rf = modelo_rf.transform(teste)\n\nreg_model_evaluator(predictions_rf, labelCol=target)\n\n# Exemplos de predições feitas:\npredictions_rf.select(\"prediction\", target, \"features\").show(5)\nprint(modelo_rf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">==============================\nMétricas no conjunto de TREINO\n==============================\nMAE: 69.85585819438707\nRMSE: 89.03902063028401\nr2 Ajustado: 0.2466437605469327\nRandomForestRegressionModel (uid=RandomForestRegressor_a6fa443ac2e6) with 20 trees\n=============================\nMétricas no conjunto de TESTE\n=============================\nMAE: 70.20189846431735\nRMSE: 89.83643742424796\nr2 Ajustado: 0.24037665748992953\n+------------------+--------+--------------------+\n        prediction|target_3|            features|\n+------------------+--------+--------------------+\n 33.46886745137141|     0.0|(101,[0,4,7,10,11...|\n  81.3357035051252|     0.0|(101,[1,4,6,12,20...|\n118.37418035206358|     0.0|(101,[1,6,13,20,2...|\n   63.903027493267|     0.0|(101,[0,4,7,8,13,...|\n 46.99972405449182|     0.0|(101,[2,4,6,9,13,...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRandomForestRegressionModel (uid=RandomForestRegressor_a6fa443ac2e6) with 20 trees\n</div>"]}}],"execution_count":147},{"cell_type":"code","source":["ExtractFeatureImp(modelo_rf.featureImportances, predictions_rf, \"features\").head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>name</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>56</td>\n      <td>qtt_orders_last_year</td>\n      <td>0.229773</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>53</td>\n      <td>orders_last_91d</td>\n      <td>0.174459</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>94</td>\n      <td>sum_order_total</td>\n      <td>0.139125</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>93</td>\n      <td>sum_paid_amount</td>\n      <td>0.127746</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>84</td>\n      <td>freq_last_91d</td>\n      <td>0.077298</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>58</td>\n      <td>qtt_valid_orders</td>\n      <td>0.055115</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>64</td>\n      <td>maturity_orders</td>\n      <td>0.050119</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>81</td>\n      <td>sum_valid_order</td>\n      <td>0.031323</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>79</td>\n      <td>number_of_orders</td>\n      <td>0.030156</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>73</td>\n      <td>sum_general_net_profit</td>\n      <td>0.019965</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":148},{"cell_type":"markdown","source":["#### 8.2.4 Gradient-boosted Tree Regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed=42)\n\ngbt = GBTRegressor(labelCol=target, maxIter=10)\n\nmodelo_gbt = gbt.fit(treino)\n\nprint('==============================')\nprint('Métricas no conjunto de TREINO')\nprint('==============================')\n\npredictions_gbt = modelo_gbt.transform(treino)\n\nreg_model_evaluator(predictions_gbt, labelCol=target)\n\nprint(modelo_gbt)\n\nprint('=============================')\nprint('Métricas no conjunto de TESTE')\nprint('=============================')\n\npredictions_gbt = modelo_gbt.transform(teste)\n\n# Exemplos de predições feitas:\npredictions_gbt.select(\"prediction\", target, \"features\").show(5)\n\nreg_model_evaluator(predictions_gbt, labelCol=target)\n\nprint(modelo_gbt)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">==============================\nMétricas no conjunto de TREINO\n==============================\nMAE: 68.72929310461664\nRMSE: 88.27304226612043\nr2 Ajustado: 0.2595498427214058\nGBTRegressionModel (uid=GBTRegressor_52ffa54975e5) with 10 trees\n=============================\nMétricas no conjunto de TESTE\n=============================\n+------------------+--------+--------------------+\n        prediction|target_3|            features|\n+------------------+--------+--------------------+\n22.067178557602467|     0.0|(101,[0,4,7,10,11...|\n 69.27719522084826|     0.0|(101,[1,4,6,12,20...|\n 85.33524748391183|     0.0|(101,[1,6,13,20,2...|\n  63.7784609576873|     0.0|(101,[0,4,7,8,13,...|\n 50.63269193030488|     0.0|(101,[2,4,6,9,13,...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nMAE: 69.7274025442343\nRMSE: 89.790348342019\nr2 Ajustado: 0.241155881665866\nGBTRegressionModel (uid=GBTRegressor_52ffa54975e5) with 10 trees\n</div>"]}}],"execution_count":150},{"cell_type":"code","source":["ExtractFeatureImp(modelo_gbt.featureImportances, predictions_gbt, \"features\").head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>name</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>49</td>\n      <td>recency_days</td>\n      <td>0.127832</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>72</td>\n      <td>avg_aov_last_91d</td>\n      <td>0.112334</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>53</td>\n      <td>orders_last_91d</td>\n      <td>0.091387</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>93</td>\n      <td>sum_paid_amount</td>\n      <td>0.041000</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>94</td>\n      <td>sum_order_total</td>\n      <td>0.040154</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>56</td>\n      <td>qtt_orders_last_year</td>\n      <td>0.040108</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>77</td>\n      <td>avg_sum_view_checkout</td>\n      <td>0.036223</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>84</td>\n      <td>freq_last_91d</td>\n      <td>0.034188</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>73</td>\n      <td>sum_general_net_profit</td>\n      <td>0.030170</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>71</td>\n      <td>avg_paid_amount</td>\n      <td>0.030046</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":151},{"cell_type":"markdown","source":["#### 8.2.5 Generalize Linear Regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import GeneralizedLinearRegression\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed=42)\n\nglr = GeneralizedLinearRegression(labelCol=target, family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n\nmodelo_glr = glr.fit(treino)\n\nprint('==============================')\nprint('Métricas no conjunto de TREINO')\nprint('==============================')\n\npredictions_glr = modelo_glr.transform(treino)\n\nreg_model_evaluator(predictions_glr, labelCol=target)\n\nprint(modelo_glr)\n\nprint('=============================')\nprint('Métricas no conjunto de TESTE')\nprint('=============================')\n\npredictions_glr = modelo_glr.transform(teste)\n\n# Exemplos de predições feitas:\npredictions_glr.select(\"prediction\", target, \"features\").show(5)\n\nreg_model_evaluator(predictions_glr, labelCol=target)\n\nprint(modelo_glr)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">==============================\nMétricas no conjunto de TREINO\n==============================\nMAE: 68.9185042708861\nRMSE: 88.4801786928379\nr2 Ajustado: 0.2560707703948609\nGeneralizedLinearRegression_ea79a403ef4e\n=============================\nMétricas no conjunto de TESTE\n=============================\n+-------------------+--------+--------------------+\n         prediction|target_3|            features|\n+-------------------+--------+--------------------+\n 24.777689274363684|     0.0|(101,[0,4,7,10,11...|\n-11.267920217243018|     0.0|(101,[1,4,6,12,20...|\n  42.97813686208591|     0.0|(101,[1,6,13,20,2...|\n  94.80555858840287|     0.0|(101,[0,4,7,8,13,...|\n   51.6822474765972|     0.0|(101,[2,4,6,9,13,...|\n+-------------------+--------+--------------------+\nonly showing top 5 rows\n\nMAE: 69.16510504869471\nRMSE: 89.15158156683957\nr2 Ajustado: 0.2519142818565707\nGeneralizedLinearRegression_ea79a403ef4e\n</div>"]}}],"execution_count":153},{"cell_type":"markdown","source":["#### 8.2.6 Comparação dos Modelos"],"metadata":{}},{"cell_type":"code","source":["print('Métricas Linear Regression')\nprint('=============================')\nreg_model_evaluator(predictions_lr, labelCol=target)\n\nprint('Métricas Decision Tree')\nprint('=============================')\nreg_model_evaluator(predictions_dt, labelCol=target)\n\nprint('Métricas Random Forest')\nprint('=============================')\nreg_model_evaluator(predictions_rf, labelCol=target)\n\nprint('Métricas Gradient Boosted Tree')\nprint('=============================')\nreg_model_evaluator(predictions_gbt, labelCol=target)\n\nprint('Métricas Generalized Linear Regression')\nprint('=============================')\nreg_model_evaluator(predictions_glr, labelCol=target)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Métricas Linear Regression\n=============================\nMAE: 69.16363463013249\nRMSE: 89.15706590190861\nr2 Ajustado: 0.2518222390938737\nMétricas Decision Tree\n=============================\nMAE: 70.70507601969265\nRMSE: 90.64620537044239\nr2 Ajustado: 0.22662074884649974\nMétricas Random Forest\n=============================\nMAE: 70.20189846431735\nRMSE: 89.83643742424796\nr2 Ajustado: 0.24037665748992953\nMétricas Gradient Boosted Tree\n=============================\nMAE: 69.7274025442343\nRMSE: 89.790348342019\nr2 Ajustado: 0.241155881665866\nMétricas Generalized Linear Regression\n=============================\nMAE: 69.16510504869471\nRMSE: 89.15158156683957\nr2 Ajustado: 0.2519142818565707\n</div>"]}}],"execution_count":155},{"cell_type":"markdown","source":["### 8.3 Feature Selection using Random Forest"],"metadata":{}},{"cell_type":"code","source":["target = 'target_3'\ndataset = spark.table(\"dataset_churn_200617_sem_outliers\").repartition(2).cache()\ndataset.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: 89257</div>"]}}],"execution_count":157},{"cell_type":"code","source":["# Reference: http://people.stat.sc.edu/haigang/improvement.html\n# https://www.timlrx.com/2018/06/19/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator/\n# Random Forest\nfrom pyspark.ml.regression import RandomForestRegressor\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\nmodelo_rf = RandomForestRegressor(labelCol=target, featuresCol=\"features\")\n\n# ParamGrid para Cross Validation\n# DecisionTree currently only supports maxDepth &lt;= 30, but was given maxDepth = 40\nparamGrid = ParamGridBuilder()\\\n               .addGrid(modelo_rf.maxDepth, [15])\\\n               .addGrid(modelo_rf.maxBins, [32])\\\n               .addGrid(modelo_rf.numTrees, [20])\\\n               .build()\n\n# Evaluate Model\nevaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=target, metricName='rmse')\n\n# Cria um 5-fold CrossValidator\ncrossval = CrossValidator(estimator=modelo_rf,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)\n\n# Deve demorar ~4 min\nimport time\nti = time.time()\n# Roda cross validations\ncvModelo = crossval.fit(treino)\ntf = time.time()\nprint(\"Demorou {} segundos\".format(tf - ti))\n\nmelhorModelo_rf = cvModelo.bestModel\ncv_predicoes_rf = melhorModelo_rf.transform(teste)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\nDemorou 756.1348497867584 segundos\n</div>"]}}],"execution_count":158},{"cell_type":"code","source":["# Removendo versao antiga do modelo salvo\nmodelpath = \"/dbfs/FileStore/models/model_problema3_feature_selection_melhorModelo_rf\"\ndbutils.fs.rm(modelpath, True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: True</div>"]}}],"execution_count":159},{"cell_type":"code","source":["modelpath = \"/dbfs/FileStore/models/model_problema3_feature_selection_melhorModelo_rf\"\nmelhorModelo_rf.write().overwrite().save(modelpath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":160},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor, RandomForestRegressionModel\ntarget = 'target_3'\ndataset = spark.table(\"dataset_churn_200617_sem_outliers\").repartition(2).cache()\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\nmodelpath = \"/dbfs/FileStore/models/model_problema3_feature_selection_melhorModelo_rf\"\nsaved_melhorModelo_rf = RandomForestRegressionModel.load(modelpath)\ncv_predicoes_rf = saved_melhorModelo_rf.transform(teste)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":161},{"cell_type":"code","source":["# https://stackoverflow.com/questions/42549200/how-to-get-all-parameters-of-estimator-in-pyspark\n{param[0].name: param[1] for param in saved_melhorModelo_rf.extractParamMap().items()}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: {&#39;cacheNodeIds&#39;: False,\n &#39;checkpointInterval&#39;: 10,\n &#39;featureSubsetStrategy&#39;: &#39;auto&#39;,\n &#39;impurity&#39;: &#39;variance&#39;,\n &#39;maxMemoryInMB&#39;: 256,\n &#39;minInfoGain&#39;: 0.0,\n &#39;minInstancesPerNode&#39;: 1,\n &#39;predictionCol&#39;: &#39;prediction&#39;,\n &#39;seed&#39;: 2502083311556356884,\n &#39;subsamplingRate&#39;: 1.0,\n &#39;featuresCol&#39;: &#39;features&#39;,\n &#39;labelCol&#39;: &#39;target_3&#39;,\n &#39;maxBins&#39;: 32,\n &#39;maxDepth&#39;: 15,\n &#39;numTrees&#39;: 20}</div>"]}}],"execution_count":162},{"cell_type":"code","source":["varlist = ExtractFeatureImp(saved_melhorModelo_rf.featureImportances, cv_predicoes_rf, \"features\")\nvaridx = [x for x in varlist['idx'][0:100]]\n\nvarlist[varlist['score'] > 0].count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: idx      100\nname     100\nscore    100\ndtype: int64</div>"]}}],"execution_count":163},{"cell_type":"code","source":["# Plot feature importances (Para slide)\nimport seaborn as sns\n\nfig=plt.figure(figsize=[10,5])\nax=fig.add_subplot(111)\nax.set_title(\"Top 10 Features Random Forest\")\nax = sns.barplot(x='score', y='name',data=varlist.head(10), color=(0.2, 0.4, 0.6, 0.6))\nplt.xticks(rotation=0) \nplt.show()"],"metadata":{},"outputs":[],"execution_count":164},{"cell_type":"markdown","source":["### 8.4 Hyperparameters and Cross Validation"],"metadata":{}},{"cell_type":"markdown","source":["#### 8.4.1 Generalized Linear Regression"],"metadata":{}},{"cell_type":"code","source":["target = 'target_3'\ndataset = spark.table(\"dataset_churn_200617_sem_outliers\")#.repartition(2).cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":167},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorSlicer\n\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nmodeloGLR = GeneralizedLinearRegression(featuresCol='features_subset', labelCol=target)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(modeloGLR.family, ['gaussian'])\\\n    .addGrid(modeloGLR.link, ['identity'])\\\n    .addGrid(modeloGLR.maxIter, [25,40])\\\n    .addGrid(modeloGLR.regParam, [0.1])\\\n    .build()\n#, 'gamma', 'poisson'])\\\nevaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=target, metricName='mae')\n\ncrossval = CrossValidator(estimator=modeloGLR,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)\n\n# Deve demorar ~4 min\nimport time\nti = time.time()\ncvModelo = crossval.fit(treino_subset)\ntf = time.time()\nprint(\"Demorou {} segundos\".format(tf - ti))\n\nmelhorModelo = cvModelo.bestModel\n\ncvPrevisoes_GLR = melhorModelo.transform(teste_subset)\nreg_model_evaluator(cvPrevisoes_GLR, labelCol=target)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\nDemorou 49.85293173789978 segundos\nMAE: 69.1629070162891\nRMSE: 89.15358430772636\nr2 Ajustado: 0.25188067081666365\n</div>"]}}],"execution_count":168},{"cell_type":"code","source":["{param[0].name: param[1] for param in melhorModelo.extractParamMap().items()}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: {&#39;family&#39;: &#39;gaussian&#39;,\n &#39;featuresCol&#39;: &#39;features_subset&#39;,\n &#39;fitIntercept&#39;: True,\n &#39;labelCol&#39;: &#39;target_3&#39;,\n &#39;maxIter&#39;: 25,\n &#39;predictionCol&#39;: &#39;prediction&#39;,\n &#39;regParam&#39;: 0.1,\n &#39;solver&#39;: &#39;irls&#39;,\n &#39;tol&#39;: 1e-06,\n &#39;variancePower&#39;: 0.0,\n &#39;link&#39;: &#39;identity&#39;}</div>"]}}],"execution_count":169},{"cell_type":"code","source":["# Removendo versao antiga do modelo salvo\nmodelpath = \"/dbfs/FileStore/models/model_problema3_cvModelo_GLR_v1\"\ndbutils.fs.rm(modelpath, True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: True</div>"]}}],"execution_count":170},{"cell_type":"code","source":["# model_problema3_cvModelo_GLR_v1: com RF como método de FeatureSelction; sem estado/favorite_dishes.\nmodelpath = \"/dbfs/FileStore/models/model_problema3_cvModelo_GLR_v2\"\ncvModelo.write().overwrite().save(modelpath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":171},{"cell_type":"code","source":["target = 'target_3'\ndataset = spark.table(\"dataset_churn_200617_sem_outliers\")\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\n\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nmodelpath = \"/dbfs/FileStore/models/model_problema3_cvModelo_GLR_v2\"\nsaved_cvModelo = CrossValidatorModel.load(modelpath)\nmelhorModelo = saved_cvModelo.bestModel\ncvPrevisoes_GLR = melhorModelo.transform(teste_subset)\nreg_model_evaluator(cvPrevisoes_GLR, labelCol=target)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MAE: 68.6333663102972\nRMSE: 88.25310127166915\nr2 Ajustado: 0.2618314685137697\n</div>"]}}],"execution_count":172},{"cell_type":"markdown","source":["Removendo Outliers: </br>\nMAE: 68.6333663102972</br>\nRMSE: 88.25310127166915</br>\nr2 Ajustado: 0.2618314685137697"],"metadata":{}},{"cell_type":"markdown","source":["Com outliers: </br>\nMAE: 107.74259499274247 </br>\nRMSE: 168.929983814752 </br>\nr2 Ajustado: 0.5860352598210408 </br>"],"metadata":{}},{"cell_type":"markdown","source":["#### 8.4.2 Gradient Boosted Tree"],"metadata":{}},{"cell_type":"code","source":["target = 'target_3'\ndataset = spark.table(\"dataset_churn_200617_sem_outliers\").repartition(2).cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":176},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.feature import VectorSlicer\n\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\n\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\nmodeloGBT = GBTRegressor(featuresCol='features_subset', labelCol=target)\n\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(modeloGBT.stepSize, [0.1])\\\n    .addGrid(modeloGBT.maxDepth, [5])\\\n    .addGrid(modeloGBT.maxIter, [20])\\\n    .build()\n#     .addGrid(modeloGBT.maxBins, [30])\\\n#     .addGrid(modeloGBT.maxDepth, [10,20])\\\n#     .addGrid(modeloGBT.maxIter, [30,60])\\\n\nevaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=target, metricName='rmse')\n\ncrossval = CrossValidator(estimator=modeloGBT,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=2)\n\n# Deve demorar ~16 min\nimport time\nti = time.time()\ncvModeloGBT = crossval.fit(treino_subset)\ntf = time.time()\nprint(\"Demorou {} segundos\".format(tf - ti))\n\nmelhorModelo = cvModeloGBT.bestModel\ncvPrevisoes_GBT = melhorModelo.transform(teste_subset)\nreg_model_evaluator(cvPrevisoes_GBT, labelCol=target)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\nDemorou 507.8675220012665 segundos\nMAE: 68.89573169900193\nRMSE: 88.71178330885456\nr2 Ajustado: 0.2541384920188542\n</div>"]}}],"execution_count":177},{"cell_type":"markdown","source":["Max depth = 5, maxIter = 20, cv = 2 </br>\nMAE: 68.89573169900193</br>\nRMSE: 88.71178330885456</br>\nr2 Ajustado: 0.2541384920188542"],"metadata":{}},{"cell_type":"markdown","source":["Com maxIter = 40</br>\nMAE: 73.40236201755101 </br>\nRMSE: 96.2552937598589</br>\nr2 Ajustado: 0.1279477498234911"],"metadata":{}},{"cell_type":"markdown","source":["Com maxIter = 30</br>\nDemorou 3139.39408659935 segundos</br>\nMAE: 72.85990046883227</br>\nRMSE: 95.94414358577018</br>\nr2 Ajustado: 0.1335765446651931"],"metadata":{}},{"cell_type":"code","source":["# Removendo versao antiga do modelo salvo\nmodelpath = \"/dbfs/FileStore/models/model_problema3_cvModelo_GBT_v2\"\ndbutils.fs.rm(modelpath, True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: True</div>"]}}],"execution_count":181},{"cell_type":"code","source":["# model_problema3_cvModelo_GBT_v1: com RF como método de FeatureSelction; sem estado/favorite_dishes.\nmodelpath = \"/dbfs/FileStore/models/model_problema3_cvModelo_GBT_v3\"\ncvModeloGBT.write().overwrite().save(modelpath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":182},{"cell_type":"code","source":["dataset = spark.table(\"dataset_churn_200617_sem_outliers\").repartition(2).cache()\ntreino, teste = dataset.randomSplit([0.8, 0.2], seed = 42)\n\nmodelpath = \"/dbfs/FileStore/models/model_problema3_cvModelo_GBT_v3\"\nvector_slicer = VectorSlicer(inputCol= \"features\", indices= varidx, outputCol= \"features_subset\")\ntreino_subset = vector_slicer.transform(treino)\nteste_subset = vector_slicer.transform(teste)\n\nsaved_cvModeloGBT = CrossValidatorModel.load(modelpath)\nmelhorModeloGBT = saved_cvModeloGBT.bestModel\ncvPrevisoes_GBT = melhorModeloGBT.transform(teste_subset)\nreg_model_evaluator(cvPrevisoes_GBT, labelCol=target)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MAE: 68.60092185051593\nRMSE: 88.33054174994344\nr2 Ajustado: 0.26562979606366455\n</div>"]}}],"execution_count":183},{"cell_type":"code","source":["# https://stackoverflow.com/questions/42549200/how-to-get-all-parameters-of-estimator-in-pyspark\n{param[0].name: param[1] for param in melhorModeloGBT.extractParamMap().items()}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[22]: {&#39;cacheNodeIds&#39;: False,\n &#39;checkpointInterval&#39;: 10,\n &#39;featureSubsetStrategy&#39;: &#39;all&#39;,\n &#39;impurity&#39;: &#39;variance&#39;,\n &#39;lossType&#39;: &#39;squared&#39;,\n &#39;maxBins&#39;: 32,\n &#39;maxMemoryInMB&#39;: 256,\n &#39;minInfoGain&#39;: 0.0,\n &#39;minInstancesPerNode&#39;: 1,\n &#39;predictionCol&#39;: &#39;prediction&#39;,\n &#39;seed&#39;: -6682481135904123338,\n &#39;subsamplingRate&#39;: 1.0,\n &#39;validationTol&#39;: 0.01,\n &#39;featuresCol&#39;: &#39;features_subset&#39;,\n &#39;labelCol&#39;: &#39;target_3&#39;,\n &#39;maxDepth&#39;: 5,\n &#39;maxIter&#39;: 20,\n &#39;stepSize&#39;: 0.1}</div>"]}}],"execution_count":184},{"cell_type":"code","source":["varlist = ExtractFeatureImp(melhorModeloGBT.featureImportances, cvPrevisoes_GBT, \"features_subset\")\nvaridx = [x for x in varlist['idx'][0:100]]\n\nvarlist[varlist['score'] > 0].count()\n# Plot feature importances (Para slide)\nimport seaborn as sns\n\nfig=plt.figure(figsize=[10,5])\nax=fig.add_subplot(111)\nax.set_title(\"Top 15 Features Gradient Boosted Tree\")\nax = sns.barplot(x='score', y='name',data=varlist.head(15), color=(0.2, 0.4, 0.6, 0.6))\nplt.xticks(rotation=0) \nplt.show()"],"metadata":{},"outputs":[],"execution_count":185},{"cell_type":"markdown","source":["### 8.5 Conclusão e Resposta do Problema"],"metadata":{}},{"cell_type":"markdown","source":["Abaixo temos para a variável target, `sum_order_total`, as seguintes estatísticas:\n- número de pedidos mínimo: R$14\n- número de pedidos máximo: R$7.6k\n- mediana: R$128.7\n- média: R$207.79"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\ndef median(values_list):\n    med = np.median(values_list)\n    return float(med)\nudf_median = f.udf(median)\n\ndf_filtrado = spark.table(\"df_filtrado_200610\")\n\ndisplay(df_filtrado.agg(min('sum_order_total'),udf_median(f.collect_list(col('sum_order_total'))),max('sum_order_total'),avg('sum_order_total')))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>min(sum_order_total)</th><th>median(collect_list(sum_order_total, 0, 0))</th><th>max(sum_order_total)</th><th>avg(sum_order_total)</th></tr></thead><tbody><tr><td>14.0</td><td>128.7</td><td>7597.38</td><td>207.7945444706828</td></tr></tbody></table></div>"]}}],"execution_count":188},{"cell_type":"markdown","source":["**Resultados:** O melhor modelo para a predição do valor mensal gasto no próximo mês (resultados avaliação do modelo na base de teste) foi:\n- GBT: MAE = 68.6; RSME = 88.3\n\nPara a base, a média da target é de R$207.79  e a mediana é R$128.7. O `MAE` obtido, R$69.7, é \"apenas\" aproximadamente 25% maior que o ticket médio mensal (~R$55). Isto é, para uma base onde as pessoas gastam, em média, 4 tickets médios por mês, o modelo construído erra 1.25 pedidos.\n\nO `RMSE` é uma métrica útil pois penaliza erros muito grandes. Sem o tratamento de outliers, tínhamos um MAE de R$107 e RMSE de R$170 (RMSE 58% maior que o MAE). Ao remover os outliers e fazer o CV e Hyperparâmetros, houve uma diminuição dos erros para MAE de R$69.7 e RMSE de R$89 (RMSE 28% maior que o MAE). Ou seja, tivemos um ganho de 30p.p. apenas na comparação, além de uma diminuição de ~35% do MAE.\n\nComo o `RSME` (~R$89) ficou mais próximo do `MAE`, apesar de estarmos errando, a magnitude da distribuição dos erros não indica que temos erros muito elevados.\n\nSob uma perspectiva de negócios, ao prever qual vai ser o possível valor gasto no mês seguinte, como destacado na motivação da escolha por essa abordagem, podemos calibrar com maior assertividade o volume de envios de pushes de forma a evitar que um eventual excesso de envios de comunicações resulte em churn. Além disso, podemos também estimar melhor a demanda por região de forma a alocar melhor recursos e dar insights para o time comercial sobre regiões com potencial aumento de demanda."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":190}],"metadata":{"name":"Entrega 3 - Grupo_15 v3","notebookId":523723184344882},"nbformat":4,"nbformat_minor":0}
